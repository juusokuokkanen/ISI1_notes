%%%%%%%%  Document class  %%%%%%%%%%%%
\documentclass[10pt, twoside, a4paper]{book}

%%%%%%%%  Packages   %%%%%%%%%%%%%%%%
\usepackage{natbib}
%\usepackage{d:/laurim/biometria/lshort/src/lshort}
%\usepackage{numline}
%\usepackage{lineno}
\bibpunct{(}{)}{,}{a}{}{,}
\usepackage{amsmath,amssymb}
\usepackage{bm} 
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{color}

%\newenvironment{Rcode}[1]%
%{\scriptsize{\ttfamily{\bf The R-code for #1}} \\
%\ttfamily \noindent\ignorespaces }
%{\par\noindent%
%  \ignorespacesafterend}
  
\newenvironment{Rcode}[1]%
{\ttfamily{\bf The R-code for #1}
\scriptsize \\\par\verbatim}
{\endverbatim\par} 

\newenvironment{Rcode2}%
{\scriptsize \par\verbatim}%
{\endverbatim\par}

\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage[nolists]{endfloat}
\usepackage{amsthm}

\usepackage{times}
%\usepackage[T1]{fontenc}

\newcommand{\foldPath}{c:/laurim/tex/}

\newcommand{\beq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\sd}{sd}

\theoremstyle{definition}

\newtheorem{example}{Example}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
%\bibpunct{(}{)}{,}{a}{}{,}
\bibliographystyle{\foldPath saf}
%\bibliographystyle{alpha}
%\bibliographystyle{\foldPath wiley}
%\bibliographystyle{unsrt}

%%%%%%%%  Page Setup %%%%%%%%%%%%%%%%%%
%\topmargin      0pt
%\headheight     0pt 
%\headsep        0pt 
%\textheight   648pt 
%\oddsidemargin  0pt
%\textwidth    468pt 

%%%%%%%% Document %%%%%%%%%%%%%%%%%%%
\begin{document}
\pagenumbering{Roman}
\setlength{\baselineskip}{16pt}

%% Front matter
%\title{Forest biometrics with examples in R}
\title{Introduction to statistical inference 1}

\author{Lauri Meht\"atalo\\
	   University of Eastern Finland \\ School of Computing
	   }

\date{\normalsize \today}
\maketitle
\tableofcontents

%\section*{Preface}

%These are lecture notes for course Biometrics: R-statistics in Forest Sciences. In writing these notes, I have used several sources. In writing chapter 1, the main sources have been the second edition of the classical book ``Statistical Inference'' by George Casella and Roger L Berger\citep{Casellaandberger2002} and ``Methods for Forest Biometrics'' by Juha Lappi  \citep{Lappi1993}. In addition, I have used lecture notes of courses on mathematical statistics (by Eero Korpelainen) and statistical inference (by Jukka Nyblom). For chapters 2 and 3, the main sources have been the lecture notes of Jukka Nyblom on Regression analysis and Linear models, the book of Jose Pinheiro and Douglas Bates on Linear models with R \citep{Pinheiroandbates2000}, and the book ``Generalized, Linear and Mixed Models'' by CharlesMcCulloch and Shayle R Searle \citep{McCullochandsearle2001}. For the GLMs and GLMMs, I have used \citet{McCullochandsearle2001}, too. The last chapter on models systems is based mainly on book ``Econometric analysis'' by William H Greene \citep{Greene1997}. In addition, good sources of information have been the lecture notes of Annika Kangas on Forest Biometrics, the books of of Julian Faraway \citep{Faraway2004, Faraway2006}, Keith E Muller and Paul W Stewart \citep{Mullerandstewart2006}, and William N. Venables and Brian D. Ripley \citep{Venablesandripley2002}.   

\mainmatter
\pagenumbering{arabic}

\chapter{Preliminaries}
\section{Introduction}
\begin{itemize}
  \item Field of statistics builds on probability theory
  \begin{quotation}
  \textit{``You can, for example, never foretell what any one man will do, but
  you can say with precision what an average number will be up to. Individuals vary,
  but percentages remain constant. So says the statistician.''} - Sherlock Holmes
  \end{quotation}
  \item The paragraph includes the important ideas of the statistical model:
  \begin{itemize}
    \item The percentage $p$ = the model of the process or underlying
    population
    \item The behavior of individuals = data
  \end{itemize}
  \item Assuming a constat probability $p$ may be a too simplistic or naive
  assuption, and may be replaced by more realistic one where p is a function of
  the propabilities of the individual and the context where s/he is.
  \item We also need to specify a model, for the variability of the
  individuals around the $p$ to complete the model formulation.
  \begin{itemize}
    \item A crude summary if the variance-covariance matrix of the
    observations.
    \item A complete definition is done by specifying the joint distribution
    of all individuals.
  \end{itemize}
  \item We also may want to estimate how accurately we finally estimated $p$
  writing the available data
  \item The theoretical process that generates the data is called
  \begin{itemize}
    \item Statistical model  or (tilastollinen malli)
    \item Stochastic process  or (stokastinen prosessi)
    \item Random process or (satunnaisprosessi)
  \end{itemize}
  \item The process is random/stochastic because the ``man'' do not behave
  exactly according to model.\footnote{This is what is done on this part of the
  course (ISI1).}
  \begin{itemize}
    \item Probability calculus and the theory of random variables provide
    tools to formulate and understand such models.
  \end{itemize}
  \item Once model has been formulated or specified (muotoiltu), observed data
  can be used to\footnote{This is what is done on second part of the course
  (ISI2)}
  \begin{itemize}
    \item estimate model parameters
    \item evaluate the model fit (mallin sopivuus)
    \item evaluate the inaccuracy related to the estimated model parameters
  \end{itemize}
  \item When talking about models, we can talk about
  \begin{itemize}
    \item True model (Tosi malli)
    \item Estimated model (Estimoitu malli)
    \item True model always stays the same, but as data used to formulate the
    estimated model gets larger, the estimated model gets closer to true model.
    \item See example R-script \verb#regsimu.R#
  \end{itemize}
\end{itemize}
\section{Set theory}
\begin{itemize}
	\item Consider a statistical experiment (e.g. rolling a dice, measuring the
	diameter of a tree, tossing a coin, measuring the photosynthetic activity in plant etc.)
\end{itemize}
\begin{definition}
All possible outcomes of a particular experiment (koe) form a set
(joukko) called sample space (otosavaruus), denoted by S. For example:

% \textbf{Examples} % TODO: Can this be done better?
\begin{itemize}
  \item[A] Toss of a coin; $ S = \{H, T\} $
  \item[B] Reaction time, Waiting time; $ S = [0, \infty) $
  \item[C] Exercise score of this course; $ S = \{0,1,2,\ldots,210\} $
  \item[D] Number of points (events) within fixed area; $ S = \{0,1,2,\ldots $
  \item[E] CO\textsubscript{2} uptake within 0.5 hours in fixed area plot; $ S =
  (-\infty, \infty) $
  \item[F] Waiting time up to one hour (in minutes); $ S = [0, 60) $
\end{itemize}

Sample space can be countable (numeroituva) or uncountable (ylinumeroituva). If
the elements of a sample space can be put into one-to-one correspondence with a
finite subset of integers, the space is countable. Otherwise, it is uncountable.
\end{definition}
\begin{itemize}
  \item Note: Examples A and C before are countable, the others are uncountable
  \item Note: If the waiting time in G are rounded to the
  minute / second / millisecond / microsecond, the sample space becomes
  countable.
\end{itemize}
\begin{definition}
An event (tapaus) is any collection of possible outcomes of an experiment,
meaning it is a subset of S. Event A is said to occur, if the outcome of the experiment is in set A.
\end{definition}
\begin{example}
Draw a card from standard deck.
$$S = \{ \heartsuit , \diamondsuit , \clubsuit, \spadesuit \}$$
One possible event is $A = \{ \heartsuit , \diamondsuit\}$.
Another possible event is $B = \{ \diamondsuit , \clubsuit, \spadesuit \}$. The
union (unioni) of the two events includes all elements of both
$$A \cup B = \{ \heartsuit , \diamondsuit , \clubsuit, \spadesuit \}$$
The intersection (leikkaus) includes elements that are common to both events
$$A \cap B = \{ \diamondsuit \}$$
The complement (komplementti) of a set includes al elements of $S$ that are not
included in $A$
$$A^c = \{ \clubsuit, \spadesuit \}$$
Events $A$ and $B$ are said to be disjoint (erillisi\"a), if 
$$A \cap B = \emptyset$$
Number of events $A_1$, $A_2$, $A_3$, \ldots are said to be pairwise disjoint,
if
$$A_i \cap A_j = \emptyset$$
for all pairs of i, j. In addition, if $\bigcup_{i=1}^{\infty}A_i = S$,
then $A_1$, $A_2$, $A_3$, \ldots defines a partition of the sample space.
\end{example}
\begin{example}
Events $A = \{ \heartsuit , \diamondsuit\}$ and $B = \{\clubsuit, \spadesuit
\}$ are disjoint since 
$$A \cap B = \emptyset$$
Events $A_1=\{\heartsuit,
\diamondsuit \}$, $A_2 = \{ \clubsuit \}$ and $A_3 = \{ \spadesuit \}$
are pairwise disjoint. Also, since $$\bigcup_{i=1}^{3} = A_1 \cup A_2 \cup A_3 =
\{\heartsuit , \diamondsuit , \clubsuit, \spadesuit\} = S$$
they are also partiotion of S.
\end{example}
% TODO: Do we something about the dice example? In pdf notes, just example of
% sample space of dice throw experiment, and how it is numerated. {1,2,3,4,5,6}
\begin{definition}
Probability (todennäköisyys)

If a certain experiment is performed number of times (or infinite number of
times), it may lead to different outcome, which is an event of the sample space.
This frequency of outcome of an event is called probability.

For an event $A \subset S$ in an experiment, notation $P(A)$ (or $Pr(A)$)
specifies the probability of outcome / event $A$.
\end{definition}
\begin{theorem}
Axioms of probability
\begin{itemize}
  \item[1.]For every event $A$, $P(A) \neq 0$ (meaning every event is possible)
  \item[2.]$P(S) = 1$ (because something will be observed)
  \item[3.]For a sequence of pairwise disjoint
  events,$$P(\bigcup_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty}P(A_i)$$
\end{itemize}
\end{theorem}
\begin{example}
%TODO: There were some diagrams related to these examples, that are not yet
% added. Should they be?
Assume we have two events $A_1$, $X \in A_1$ and $A_2$, $X \in A_2$, which have
probabilities $P(A_1) = 0.2$ and $P(A_2) = 0.3$.

%\textbf{Case 1}%
If the events are disjoint ($A_1 \cap A_2 = \emptyset$),
% then
probability for the union of events is $$P(A_1 \cup A_2) = P(A_1) + P(A_2) = 0.2
+ 0.3 = 0.5$$

%\textbf{Case 2}%
If events are not disjoint ($A_1 \cap A_2 \neq \emptyset$), then
$$P(A_1 \cup A_2) \neq P(A_1) + P(A_2) = 0.2 + 0.3 = 0.5$$
\end{example}
\begin{example}
In a fair deck, define events $$A_1 = \{\heartsuit\}, A_2 = \{\diamondsuit\},
A_3 = \{\clubsuit\}, A_4 = \{\spadesuit\}$$which have probabilities
$$P(A_1) = P(A_2) = P(A_3) = P(A_4) = 1/4$$
Events $A_1 \ldots A_4$ are disjoint. Therefor,
$$B = A_1 \cup A_2 = \{\heartsuit, \diamondsuit\}$$
$$P(B) = P(A_1 \cup A_2) = P(A_1) + P(A_2) = 1/4 + 1/4 = 1/2$$
\end{example}
\begin{theorem}
Consider events A and B
\begin{itemize}
  \item[1.]$P(A^c) = 1 - P(A)$
  \item[2.]$P(B \cap A^c) = P(B) - P(A \cap B)$
  \item[3.]$P(A \cup B) = P(A) + P(B) - P(A \cap B)$
  \item[4.]If $A \subset B$, then $P(A) \leq P(B)$
\end{itemize}
\textbf{Note} Consider case 3 of theorem \ref{Theorem2}
$$P(A \cup B) = P(A) + P(B)-P(A \cap B)$$
$$P(A \cup B) \leq 1$$
$$P(A) + P(B)-P(A \cap B) \leq 1$$
$$P(A \cap B) \geq P(A) + P(B)-1$$
This equation is called the Bonferroni inequality. Idea is, that if we have
intersection of two events ($A$ and $B$), the probability of the intersection
can be shown to be higher or equal than the right term.

$$P(A \cap B) \geq P(A) + P(B)-1$$
Supose $A$ and $B$ are two events that occur with probability $P(A) = P(B) =
0.975$. Then $P(A \cap B) = 0.975 + 0.975 - 1 = 0.95\ldots$ 


\label{Theorem2}
\end{theorem}
\begin{theorem}
\begin{itemize}
  \item[a)] $P(A) = \sum_{i=1}^{\infty} P(A \cap C)$ for any partition
  $C_1, C_2, \ldots$
  \item[b)]$P(\bigcup_{i=1}^{\infty}A_i) \leq \sum_{i=1}^{\infty}P(A_i)$, for
  any sets $A_1, A_2, \ldots$
\end{itemize}
\end{theorem}
\begin{definition}
IF $A$ and $B$ are events in a sample space $S$ and $P(B) > 0$, then
conditional probability (ehdollinen todenn\"akoisyys) of $A$ given $B$ is
$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$
\end{definition}
\begin{example}
\label{clinicalExample1}
Clinical trial. Assume  that we know the probabilities that are represented in
table \ref{probTable1}.
Let event $A$ be the event ``the patient recovered'' with sample space $S =
\{ ``OK", ``NOT~OK"\}$, and event B is the event ``The patient was treated
with placebo'' with $S = \{``YES'', ``NO''\}$.

Now $P(A \cap B)$ can be computed directly using the table \ref{probTable1}, and
it is
$$P(A \mid B) = P(``OK'' \mid ``Placebo'') = \frac{P(A \cap B)}{P(B)} =
\frac{0.160}{0.227} \approx 0.70$$

\begin{table}
\caption{Probabilities related to clinical trial. Four different drugs and the
probabilities that patient got or did not got ok after using it.}
\label{probTable1}
\begin{tabular}{| r | r | r | r | r | r |}
\hline
{ } & Drug1 & Drug2 & Drug3 & Placebo & Total \\
\hline
OK & 0.120 & 0.087 & 0.147 & 0.160 & 0.513 \\
\hline
{NOT OK} & 0.147 & 0.167 & 0.107 & 0.067 & 0.487 \\
\hline
Total & 0.267 & 0.253 & 0.253 & 0.227 & 1.000 \\
\hline
\end{tabular}
\end{table}
\end{example}
\begin{definition}
%TODO: Can equations here be done more better?
Bayes rule

Assuming we have event $A$ and $B$,
$$P(A|B) = \frac{P(A \cap B)}{P(B)} ~~\|* P(B)$$
if $P(B) > 0$
$$P(A \cap B) = P(B)P(A|B)$$

And the other way around
$$P(B|A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A \cap B)}{P(A)} ~~\|* P(A)$$
if $P(A) > 0$
$$P(A \cap B) = P(A)P(B|A)$$
and by using the earlier $P(A \cap B) = P(B)P(A|B)$
$$P(B)P(A|B) = P(A)P(B|A) ~~\|: P(B)$$
$$P(A|B) = \frac{P(A)P(B|A)}{P(B)}$$
This is known as Bayes rule.
\end{definition}
\begin{definition}
Two events $A$ and $B$ are independent, if
$$ P(A \cap B) = P(A)P(B)$$
If two events are known to be independent, we can compute $P(A \cap B)$ as
$P(A)P(B)$
\end{definition}
\begin{example}
In example \ref{clinicalExample1} we had $P(A) = 0.513$, $P(B)=0.227$ and
$P(A \cap B) = 0.160$. By calculating
$$P(A)P(B) = 0.513 * 0.227 = 0.116 \neq 0.160 = P(A \cap B)$$
we can determine that events $A$ and $B$ are not independent.
\end{example}
\chapter{Random variables}
\section{Random variables}
\begin{definition}
A random variable (RV for short) (satunnaismuuttuja) is a function from sample
space S into the real numbers.\\
\textbf{Note} If S includes only real numbers, the definition of the RV is
implicit. Some examples of this in table \ref{tableImplicitEventRV1}. Examples
of events that not implicitly connected to random variables in table
\ref{tableImplicitEventRV2}
\newline
%TODO: Can we make the aligment of table cells better?
\begin{table}
\caption{Examples of implicit connection between event and random variable}
\label{tableImplicitEventRV1}
\begin{centering}
\begin{tabular}{p{6cm} | p{6cm}}
\textbf{Experiment} & \textbf{Event = Random variable} \\ \hline
Toss of two dice. & $X$ = Sum of two numbers \\ \hline
Estimated regression from n observations from certain true model.
& $X$ = Estimate of slope coefficient $\hat{\beta}_2$ \\ \hline
Event location in space. & $X$ = \# of events in certain fixed subset of the
space.
\end{tabular}
\end{centering}
\end{table}

\begin{table}
\caption{Examples of implicit connection between event and random variable}
\label{tableImplicitEventRV2}
\begin{centering}
\begin{tabular}{p{3cm} | p{5cm} | p{4cm}}
\textbf{Experiment} & \textbf{S} & \textbf{RV} \\ \hline
Toss of a coin & $\{``H'', ``T''\}$ & \begin{equation*}
X_1 = \left\{
\begin{array}{rl}
1 & \text{if Head}, \\
0 & \text{if Tails}
\end{array} \right
\end{equation*}
or
\begin{equation*}
X_2 = \left\{
\begin{array}{rl}
10 & \text{if Head}, \\
2 & \text{if Tails}
\end{array} \right
\end{equation*}
\\ \hline
Health status of tree & $\{``Healthy", ``Sick", ``Dead"\}$ & \begin{equation*}
X = \left\{
\begin{array}{rl}
1 & \text{if Healthy}, \\
2 & \text{if Sick}, \\
3 & \text{if Dead}
\end{array} \right
\end{equation*}
or we can define two random variables
\begin{equation*}
X_1 = \left\{
\begin{array}{rl}
0 & \text{if Healthy or Dead}, \\
1 & \text{if Sick}
\end{array} \right
\end{equation*}
\begin{equation*}
X_2 = \left\{
\begin{array}{rl}
0 & \text{if Healthy or Sick}, \\
1 & \text{if Dead}
\end{array} \right
\end{equation*}
\\
\end{tabular}
\end{centering}
\end{table}
The probability function of the original sample space is defined ot the RV as
follows:

Consider the sample space $S = {S_1, S_2, \ldots ,S_n}$, and has probability
function P. Define RV $X$ with sample space / range / support set
$$ \mathcal{X} = \{x_1, x_2, \ldots , x_n \}$$ 
% TODO: Is mathcal font good to use with RV supports?.
$$P_X(X = x_i) = P(\{ s_j \in S : X(s_j) = x_i \})$$
\textbf{Note} About the notations: Uppercase (capital) letters are used
for a random variables and lowercase letters for the realized value or range. 
$X = x$ means, that RV $X$ gets value $x$.
\end{definition}
\newpage
\begin{example}
\label{exampleThreeCoinToss}
Tree tosses of a coin. Let RV $X$ be the \# of heads, or RV $Y$ is the number of
tails. For RV $X$, well get space $$\mathcal{X} = \{0,1,2,3\}$$ See table
\ref{exampleThreeCoinTossTable1} to see how the sample space values are
converted to random variable values.
We can then calculate the probabilities that random variable gets certain value
from its support space. For random variable $X$, these probabilities are
%TODO: Better way to add space between tabular and text?
\newline
\newline
\begin{tabular}{r | r r r r}
$x$ & 0 & 1 & 2 & 3 \\
$P(X=x)$ & $\frac{1}{8}$ & $\frac{3}{8}$ & $\frac{3}{8}$ & $\frac{1}{8}$
\end{tabular}
\newline
\newline
Now, we can get the probability of event, that with three coin tosses,
we get only one heads, by getting the probability that random variable $X$
receives value 1 $$P(X=1) = P(\{HTT,~THT,~TTH\}) = \frac{1}{8}+\frac{1}{8}+\frac{1}{8} =
\frac{3}{8}$$
\end{example}
\begin{table}
\caption{Example \ref{exampleThreeCoinToss} experiment's sample
space value conversion to RV values.}
\label{exampleThreeCoinTossTable1}
\begin{tabular}{ r | r  r  r  r  r  r  r  r }
$s_i$ & HHH & HHT & HTH & HTT & THH & THT & TTH & TTT \\
\hline
$X(s_i)$ & 3 & 2 & 2 & 1 & 2 & 1 & 1 & 0 \\
$Y(s_i)$ & 0 & 1 & 1 & 2 & 2 & 1 & 2 & 3 \\
\hline
\end{tabular}
\end{table}
\begin{example}
Waiting time in a phone service. Sample space for experiment is
$$S = [0,60)$$
For RV $X$, space will be
$$\mathcal{X} = [0, 60)$$
meaning since the original experiment has sample space that is defined by real
number, the experiment directly specifies the random variable
 $$X(s) = s$$
\end{example}
\section{Cumulative distribution function}
\begin{itemize}
  \item Starting point with any computation of probability is the distribution
  function.
\end{itemize}
\begin{definition}
The culmulative distribution function (kertym\"afunktio) or just distribution
function (cdf for short) is defined as
\begin{equation*}
F_X(x) = P_X(X \leq x)~\text{for all $x$}
\end{equation*}
\end{definition}
\begin{example}
Tossing of 3 coins. Random variable $X$ = the \# of heads. Cumulative
distribution function for RV $X$ can be defined as
\begin{equation*}
F_X(x) = \left\{
\begin{array}{rll}
0 & \text{if } -\infty < x < 0 & x \in (-\infty, 0) \\
\frac{1}{8} & \text{if } 0 \leq x < 0 & x \in [0, 1) \\
\frac{1}{2} & \text{if } 1 \leq x < 0 & x \in [1, 2) \\
\frac{7}{8} & \text{if } 2 \leq x < 0 & x \in [2, 3) \\
1 & \text{if } 3 \leq x < \infty & x \in [3, \infty) 
\end{array} \right
\end{equation*}
%TODO: There is a plot about the cdf and also some specification how value for
% 1/2 and 7/8 are gotten. Are they needed?
\textbf{Notes}
\begin{itemize}
  \item[-] $F_X(x)$ is defined for all $x$!
  %TODO: If the plot for cdf is added, here is additional note.
  %\item[-] $F_X(x) has jumps at x_i \in \chi$. The jumps equal to P(X=x_i).
  \item[-] $F_X(x) = 0$ for $x < 0$ and $F_X(x) = 1$ for $x \geq 3$.
  \item[-] $F_X(x)$ is right-continuous: the value of $F_X(x)$ at the jump is
  the one we get by approaching the jump from the right.
  \item[-] $F_X(x)$ is discontinuous (in this example).
\end{itemize}
\begin{theorem}
\label{rulesForCdfTheorem}
Function $F(x)$ is a cdf if and only if the following conditions hold:
\begin{itemize}
  \item[a)] $\lim_{x \rightarrow -\infty}F(x) = 0$ and $\lim_{x \rightarrow
  \infty}F(x) = 1$.
  \item[b)] $F(x)$ is non-decreasing (kasvava) (meaning value of function
  is either increasing or constant as $x$ increases)
  \item[c)] $F(x)$ is right-continuous, i.e. for every $x_0$ 
  $$\lim_{x \downarrow x_0}F(x) = F(x_0)$$
\end{itemize}
\end{theorem}
\begin{itemize}
  \item If a function is said to be a cdf, it has to fulfill the rules a, b and
  c of theorem \ref{rulesForCdfTheorem}.
  \item If a function fulfills rules a, b and c of theorem
  \ref{rulesForCdfTheorem}, then it can be used as a cdf.
\end{itemize}
\begin{theorem}
A random variable $X$ is said to be continuous, if $F_X(x)$ is a continuous
function of x (meaning the function foes hot have jumps).
\end{theorem}
\textbf{Note} The $X$ in example \ref{exampleThreeCoinToss} was discrete.
\end{example}
\begin{example}
\label{exampleUniformDistribution1}
An example of a continuous cdf is the uniform distribution. Let RV $U$ have
equal probabilities within $\mathcal{U} \in [a, b]$. The cdf of $U$ is
\begin{equation*}
F_U(u) = \left\{
\begin{array}{rll}
0 & \text{if } u < a & u \in (-\infty, a) \\
\frac{u-a}{b-a} & \text{if }  a \leq u < b & u \in [a, b) \\
1 & \text{if } u \geq b & u \in [b, \infty) 
\end{array} \right
\end{equation*}
%TODO: Should we add the plot for the cdf here?
\end{example}
\begin{itemize}
  \item The cdf can be used for calculation of the probabilities. In general
  $$P(x \in (l, u]) = F(u) - F(l)$$
\end{itemize}
\begin{example}
Consider the tossing of 3 coins (example \ref{exampleThreeCoinToss}).
Probability that more than 1 heads is observed is $$P(X \in (1,3]) =
F_X(3)-F_X(1) = 1 - 0.5 = 0.5$$
This result is equal to
$$P(X=2)+P(X=3)=\frac{3}{8}+\frac{1}{8} = \frac{4}{8} = \frac{1}{2}$$
\end{example}
\begin{example}
Consider uniform distribution where $a=0$ and $b=2$.
$$P(X \in (0.5, 1]) = F(1) - F(0.5) = 0.5 - 0.25 = 0.25$$
\end{example}
\textbf{Note} For continuous RV %TODO: Should this be elaborated more?
$$P(x \in [a,b]) = P(x \in (a,b)) = P(x \in (a, b]) = P(x \in [a, b))$$
\begin{definition}
RVs $X$ and $Y$ are said to be identically distributed (samoin jakautuneita), if
$F_X(x) = F_Y(x)$ for every x.
%TODO: Do we need plot here?

Two RVs are said to be independent and identically distributed (i.i.d.) if they
are both independent and identically distributed.
\end{definition}
\section{Probability mass and density functions}
\begin{itemize}
  \item The probability mass function (pmf) is defined only for discrete RVs as follows.
$$f_X(x) = P(X=x)~\text{for all }x$$
\item The pmf is connected with cdf as follows.
$$F_X(x) = \sum_{k=1}^{X}f_X(k)$$
\end{itemize}
\begin{example}
\label{exampleBernoulliDistribution1}
The $Bernoulli(p)$ distribution is used for a discreate RV with two possible
values (i.e. it is the distribution for so called Bernoulli trial, which has
two possible outcomes: success (``S'') and failure (``F'')).
Record the outcomes as a RV:

\begin{tabular}{ l | l }
s_i & X(s_i) \\ \hline
``S'' & 1 \\
``F'' & 0
\end{tabular}

The pmf of $X$ is
\begin{equation*}
F(x; p) = \left\{
\begin{array}{ll}
1-p & x = 0 \\
p & x = 1 \\
0 & \text{elsewhere} 
\end{array} \right
\end{equation*}
The Bernoulli cdf is
\begin{equation*}
F(x; p) = \left\{
\begin{array}{ll}
0 & x < 0 \\
1-p & 0 \leq x < 1 \\
1 & x \geq 1 %TODO: Is this correct? In notes was \geq 0?
\end{array} \right
\end{equation*}
\textbf{Note} $a$ and $b$ for the uniform distribution (like in example
\ref{exampleUniformDistribution1}) and the $p$ in case of Bernoulli distribution
(like in example \ref{exampleBernoulliDistribution1}) are called parameters.
\end{example}
%TODO: Is the plot needed?
\begin{itemize}
  \item pmf cannot be defined for a continuous RV since it holds
  $$P(X=x) \leq \lim_{\epsilon \downarrow 0} [F_X(x) - F_X(x - \epsilon)] = 0$$
  \item For continuous RV, the sum of pmf is replaced by integral\ldots
  $$P(X \leq x) = F_X(x) = \int_{-\infty}^{x} f_X(t)dt$$
  \item \ldots which also implies
  $$\frac{d}{dx}F_X(x)=f_X(x)$$
\end{itemize}
\begin{definition}
Function $f_X(x)$ above is called probability density function (pdf for short)
(tiheysfunktio).
\end{definition}
\begin{theorem}
A function $f(x)$ is a pdf or pmf, if
\begin{itemize}
  \item[a)]$f(x) \geq 0 $ for all x
  \item[b)]$$\sum_X f(x) = 1~\text{(if pmf)}$$
  or
  $$\int_{-\infty}^{\infty} f(x) dx = 1~\text{(if pdf)}$$
\end{itemize}
%TODO: Are the plots for pmf and pdf needed here?
\end{theorem}
\textbf{Note} Notation $\sim$ means ``is distributed as''; for example
\begin{itemize}
  \item $X \sim f(x)$
  \item $X \sim F(x)$
  \item $X \sim Bernoulli(p)$
\end{itemize}
\begin{example}
Perhaps the most widely used continuous distribution is the Normal distribution,
which has support $(-\infty, \infty)$, and pdf
$$f(x; \mu, \sigma) =
\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},~-\infty < \mu <
\infty,~\sigma > 0$$
If $\mu = 0$ and $\sigma = 1$, then function is called standard normal density
(standardoitu normaali jakauma).
%TODO: Is the plot here needed?
The cdf of normal distribution 
$$\int_{-\infty}^{x}f_X(t)dt =
\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{x}e^{-\frac{(t-\mu)^2}{\sigma^2}}dt=\ldots$$
%TODO: is the \sigma^2 correct above or is it just \sigma?
but it cannot be written in closed form. However, R and other software can
compute the value numerically.

\textbf{Note} Parameters: $\mu$ is the expected value, and $\sigma$ is the
standard deviation of X.
\end{example}
\begin{example}
Let $U \sim Uniform(0, 10)$ and X be the sample mean
$\frac{\sum_{i=1}^{n}u_i}{n}$ i n a sample of size n. R-script \verb#unifmean.R#
illustrates, that if $n$ is sufficiently large, then
$$X \sim N(\frac{10}{2}, \frac{10/\sqrt{12}}{n})$$
This is a consequence of central limit theorem.
\end{example}
%TODO: Is the bernoulli example needed (multiple left/right bernoulli
% experiments from normal distribution)? Leaving it out for now, since relies so much in the
% visualization.
\textbf{Note} Often the parameters of a distribution are functions, not singe
numbers. For example, the probability of success, $p$, in the Bernoulli case
may depend on some fixed/known characteristics $x$ of sampling  unit, ie. we
have $p(x)$ instad of the p. Or the mean of normally distributed RV may also depend
on $x$, so we use $\mu(x)$ instead of $\mu$. Examples 1.1. and 1.2. in
\verb#notes.pdf# demonstrate this
\begin{example} $p(Age) = 0.1 + 0.0005*Age$
\begin{itemize}
  \item [If] $Age = 0$, then $p(0) = 0.1$.
  \item [If] $Age = 10$, then $p(10) = 0.15$
  \item [If] $Age = 100$, then $p(10) = 0.1 + 0.5 = 0.6$
\end{itemize}
Since $p(Age)$ as specified above may become negative or higher than 1 with some
values of the parameter $Age$, function $p(x)$ should be defined so, that it
can only get values within $(0,1)$. This could be achieved with binary logistic
model.
\end{example}
%TODO: Should we have these two examples (one above and one bellow)?
%\begin{example} We have normal distributed random variable $Y \sim N(\mu(x),
%\sigma)$, where
%$\mu(x) = 0.5*x$, where $x$ is tree age and $\sigma = 2$. $Y$ specifies the
%diameter of the tree.
%
%\end{example}
\section{Transformations of a random variable}
\begin{itemize}
  \item If $X$ is a random variable, then any function of it, e.g. $Y=g(x):
  \mathcal{X} \rightarrow \mathcal{Y}$ is also a random variable, where the
  function $g(x)$ is called transformation.
  \item The probabilistic properties of Y can be expressed using those of X:
  $$P(Y \in A) = P(g(x) \in A)$$
  \item If $g(x)$ is monotonic function in $X$, we can define the inverse
  function $g^{-1}(y): \matchcal{Y} \rightarrow \mathcal{X}$ and establish the
  theorem \ref{theoremTransformations1}.
\end{itemize}
\begin{theorem}
\label{theoremTransformations1}
Let $X$ have cdf $F_X(x)$. Let $Y = g(x)$ and $\mathcal{X} = \{x, f_X(x) > 0\}$
and $\mathcal{Y} = \{y: y=(g(x)~\text{for some } x \in \mathcal{X}\}$
\begin{itemize}
  \item[a)] If $g$ is an increasing function of $\mathcal{X}$, then 
  $$F_Y(y) = F_X(g^{-1}(y))\text{ for }y \in \mathcal{Y}$$
  \begin{itemize}
    \item If $g(x)$ specifies $y(x)$, we solve $y(x) = y$ for $x$ to write $x$
    in terms of $y$ $x(y) = g^{-1}(y)$.
  \end{itemize}
  \item[b)] If $g$ is decreasing function of $\matchcal{X}$ and $X$ is
  continuous, then 
  $$F_Y(y) = 1 - F_X(g^{-1}(y))$$
\end{itemize}
\end{theorem}
\begin{example}
Suppose $X \sim Uniform(0, 1)$. Now $F_X(x) = x,~ 0 < x < 1$. Consider
transformation $Y=g(x) = -ln(x)$.
\begin{itemize}
  \item $ln(x)$ is decreasing, since $\frac{d}{dx}(-ln(x)) = -\frac{1}{x} < 0$
  when $x > 0$
  $$\mathcal{X} \in (0,1) \rightarrow \mathcal{Y} \in (0, \infty)$$
\item Because $g(x)$ is decreasing, the minimum of $Y$ will be obtained at the
maximum value of $X$.
$$-ln(1) = 0,~\text{(minimum of $Y$)}$$
\item The maximum of $Y$ is obtained at the minimum of $X$.
$$-ln(x) \rightarrow \infty,~\text{as } x \rightarrow \infty $$
\item We can get the $g^{-1}$ by solving $x$ for $y = g(x)$
$$y = -ln(x)~~||*(-1)$$
$$-y = ln(x)$$
$$x = e^{-y} = g^{-1}(y)$$
\item The cdf of Y is
$$F_Y(y) = 1 - F_X(g^{-1}(y)) = 1 - F_X(e^{-y}) = 1 - e^{-y}$$
\item The pdf of Y is
$$f_Y(y) = \frac{dF_Y(y)}{dy} = \frac{d(1-e^{-y})}{dy} = -e^{-y}(-1)=e^{-y}$$
%TODO: In notes, it was exponential pdf, but capital F is suposed to mean cdf.
\item Note: $F_Y(y)$ is called the exponential cdf with the rate parameter 1.
$f_Y(y)$ is the corresponding density.
\end{itemize}
%TODO: Two plots on page 2 of notes 5. Should they be added?
\end{example}
\begin{theorem} Let $X \sim f_X(x)$ and $Y=g(X)$ be a monotone transformation,
and supports $\mathcal{X} = \{x, f_X(x) > 0\}$
and $\mathcal{Y} = \{y: y=(g(x)~\text{for some } x \in \mathcal{X}\}$ (like in
theorem \ref{theoremTransformations1}). Suppose that $f_X(x)$ is continuous on
$\mathcal{X}$ and $g^{-1}(y)$ has continuous derivative on $\mathcal Y$.

The pdf of $Y$ is
\begin{equation*} f_Y(y) =
\left\{
\begin{array}{ll}
f_X(g^{-1}(y)) \vert \frac{d}{dx}g^{-1}(y) \vert & y \in \mathcal{Y} \\
0 & otherwise
\end{array}
\end{equation*}
\end{theorem}
%TODO: Should we have the example here (that just tells to look the previous)?
\begin{definition}
Let $X$ have cdf $F(x)$. Quartile function of $X$ is defined as the inverse of
cdf:
$$q(u) = F^{-1}(u)$$
\end{definition}
\begin{example}
We want to find a value of $X$ such that $P(X \leq x) = 0.95$. The solution is
given by $q(x) = q(0.95) = \ldots = x_*$ %TODO: Is this correct?
\end{example}
\begin{example}
Random number generator. Let $q_x(u)$  be the quartile function of $X$. If we
are able to generate a sample from $U \sim Uniform(0,1)$ distribution,
transformation $g(u) = q_X(u)$ provides a random sample from the distribution of $X$.
\begin{equation*} F(x) =
\left\{
\begin{array}{lll}
0 & x \leq 0 & \\ %TODO: In the notes was x \leq x but used x \leq 0. Correct?
0.1x & \text{when }0 < x \leq 5 & (*) \\
0.25 + 0.05x & \text{when }5 < x \leq 15 & (**) \\
1 & x > 15
\end{array}
\end{equation*}
Then, the quartile functions for parts (*) and (**) are solved from $F(x) = u$
by solving x. 

Part (*)
$$0.1x = u$$
$$x = 10u$$
$$\text{ for } F(0) < u \leq F(5) \rightarrow 0 < u \leq 0.5$$

Part (**)
$$0.25+0.05x = u$$
$$x = 20u - 5$$
$$\text{ for } F(5) < u \leq F(15) \rightarrow 0.5 < u \leq 1$$
Which provide us with the quartile function for $X$
\begin{equation*} q(u) =
\left\{
\begin{array}{lll}
10u & \text{if }u \in [0, 0.5) \\
20u-5 + 0.05x & \text{if }u \in [0.5, 1] \\
\end{array}
\end{equation*}
Quartile function $q(u)$ implemented in R-script \verb#probIntTrans.R#.
%TODO: Is the plot from page 4 of notes 6 needed?
\end{example}
\end{document}