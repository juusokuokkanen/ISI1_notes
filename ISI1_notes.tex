%%%%%%%%  Document class  %%%%%%%%%%%%
\documentclass[10pt, twoside, a4paper]{book}

%%%%%%%%  Packages   %%%%%%%%%%%%%%%%
\usepackage{natbib}
%\usepackage{d:/laurim/biometria/lshort/src/lshort}
%\usepackage{numline}
%\usepackage{lineno}
\bibpunct{(}{)}{,}{a}{}{,}
\usepackage{amsmath,amssymb}
\usepackage{bm} 
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{color}

%\newenvironment{Rcode}[1]%
%{\scriptsize{\ttfamily{\bf The R-code for #1}} \\
%\ttfamily \noindent\ignorespaces }
%{\par\noindent%
%  \ignorespacesafterend}
  
\newenvironment{Rcode}[1]%
{\ttfamily{\bf The R-code for #1}
\scriptsize \\\par\verbatim}
{\endverbatim\par} 

\newenvironment{Rcode2}%
{\scriptsize \par\verbatim}%
{\endverbatim\par}

\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage[nolists]{endfloat}
\usepackage{amsthm}

\usepackage{times}
%\usepackage[T1]{fontenc}

\newcommand{\foldPath}{c:/laurim/tex/}

\newcommand{\beq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}
\newcommand{\ud}{\,\mathrm{d}}
\newcommand{\ureal}{\,\mathbb{R}}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\sd}{sd}

\theoremstyle{definition}

\newtheorem{example}{Example}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
%\bibpunct{(}{)}{,}{a}{}{,}
\bibliographystyle{\foldPath saf}
%\bibliographystyle{alpha}
%\bibliographystyle{\foldPath wiley}
%\bibliographystyle{unsrt}

%%%%%%%%  Page Setup %%%%%%%%%%%%%%%%%%
%\topmargin      0pt
%\headheight     0pt 
%\headsep        0pt 
%\textheight   648pt 
%\oddsidemargin  0pt
%\textwidth    468pt 

%%%%%%%% Document %%%%%%%%%%%%%%%%%%%
\begin{document}
\pagenumbering{Roman}
\setlength{\baselineskip}{16pt}

%% Front matter
%\title{Forest biometrics with examples in R}
\title{Introduction to statistical inference 1}

\author{Lauri Meht\"atalo\\
	   University of Eastern Finland \\ School of Computing
	   }

\date{\normalsize \today}
\maketitle
\tableofcontents

%\section*{Preface}

%These are lecture notes for course Biometrics: R-statistics in Forest Sciences. In writing these notes, I have used several sources. In writing chapter 1, the main sources have been the second edition of the classical book ``Statistical Inference'' by George Casella and Roger L Berger\citep{Casellaandberger2002} and ``Methods for Forest Biometrics'' by Juha Lappi  \citep{Lappi1993}. In addition, I have used lecture notes of courses on mathematical statistics (by Eero Korpelainen) and statistical inference (by Jukka Nyblom). For chapters 2 and 3, the main sources have been the lecture notes of Jukka Nyblom on Regression analysis and Linear models, the book of Jose Pinheiro and Douglas Bates on Linear models with R \citep{Pinheiroandbates2000}, and the book ``Generalized, Linear and Mixed Models'' by CharlesMcCulloch and Shayle R Searle \citep{McCullochandsearle2001}. For the GLMs and GLMMs, I have used \citet{McCullochandsearle2001}, too. The last chapter on models systems is based mainly on book ``Econometric analysis'' by William H Greene \citep{Greene1997}. In addition, good sources of information have been the lecture notes of Annika Kangas on Forest Biometrics, the books of of Julian Faraway \citep{Faraway2004, Faraway2006}, Keith E Muller and Paul W Stewart \citep{Mullerandstewart2006}, and William N. Venables and Brian D. Ripley \citep{Venablesandripley2002}.   

\mainmatter
\pagenumbering{arabic}

\chapter{Preliminaries}
\section{Introduction}
\begin{itemize}
  \item Field of statistics builds on probability theory
  \begin{quotation}
  \textit{``You can, for example, never foretell what any one man will do, but
  you can say with precision what an average number will be up to. Individuals vary,
  but percentages remain constant. So says the statistician.''} - Sherlock Holmes
  \end{quotation}
  \item The paragraph includes the important ideas of the statistical model:
  \begin{itemize}
    \item The percentage $p$ = the model of the process or underlying
    population
    \item The behavior of individuals = data
  \end{itemize}
  \item Assuming a constant probability $p$ may be a too simplistic or naive
  assumption, and may be replaced by more realistic one where p is a function of
  the probabilities of the individual and the context where s/he is.
  \item We also need to specify a model, for the variability of the
  individuals around the $p$ to complete the model formulation.
  \begin{itemize}
    \item A crude summary if the variance-covariance matrix of the
    observations.
    \item A complete definition is done by specifying the joint distribution
    of all individuals.
  \end{itemize}
  \item We also may want to estimate how accurately we finally estimated $p$
  writing the available data
  \item The theoretical process that generates the data is called
  \begin{itemize}
    \item Statistical model  or (tilastollinen malli)
    \item Stochastic process  or (stokastinen prosessi)
    \item Random process or (satunnaisprosessi)
  \end{itemize}
  \item The process is random/stochastic because the ``man'' do not behave
  exactly according to model.\footnote{This is what is done on this part of the
  course (ISI1).}
  \begin{itemize}
    \item Probability calculus and the theory of random variables provide
    tools to formulate and understand such models.
  \end{itemize}
  \item Once model has been formulated or specified (muotoiltu), observed data
  can be used to\footnote{This is what is done on second part of the course
  (ISI2)}
  \begin{itemize}
    \item estimate model parameters
    \item evaluate the model fit (mallin sopivuus)
    \item evaluate the inaccuracy related to the estimated model parameters
  \end{itemize}
  \item When talking about models, we can talk about
  \begin{itemize}
    \item True model (Tosi malli)
    \item Estimated model (Estimoitu malli)
    \item True model always stays the same, but as data used to formulate the
    estimated model gets larger, the estimated model gets closer to true model.
    \item See example R-script \verb#regsimu.R#
  \end{itemize}
\end{itemize}
\section{Set theory}
\begin{itemize}
	\item Consider a statistical experiment (e.g. rolling a dice, measuring the
	diameter of a tree, tossing a coin, measuring the photosynthetic activity in plant etc.)
\end{itemize}
\begin{definition}
All possible outcomes of a particular experiment (koe) form a set
(joukko) called sample space (otosavaruus), denoted by S. For example:

% \textbf{Examples} % TODO: Can this be done better?
\begin{itemize}
  \item[A] Toss of a coin; $ S = \{H, T\} $
  \item[B] Reaction time, Waiting time; $ S = [0, \infty) $
  \item[C] Exercise score of this course; $ S = \{0,1,2,\ldots,210\} $
  \item[D] Number of points (events) within fixed area; $ S = \{0,1,2,\ldots $
  \item[E] CO\textsubscript{2} uptake within 0.5 hours in fixed area plot; $ S =
  (-\infty, \infty) $
  \item[F] Waiting time up to one hour (in minutes); $ S = [0, 60) $
\end{itemize}

Sample space can be countable (numeroituva) or uncountable (ylinumeroituva). If
the elements of a sample space can be put into one-to-one correspondence with a
finite subset of integers, the space is countable. Otherwise, it is uncountable.
\end{definition}
\begin{itemize}
  \item Note: Examples A and C before are countable, the others are uncountable
  \item Note: If the waiting time in G are rounded to the
  minute / second / millisecond / microsecond, the sample space becomes
  countable.
\end{itemize}
\begin{definition}
An event (tapaus) is any collection of possible outcomes of an experiment,
meaning it is a subset of S. Event A is said to occur, if the outcome of the experiment is in set A.
\end{definition}
\begin{example}
Draw a card from standard deck.
$$S = \{ \heartsuit , \diamondsuit , \clubsuit, \spadesuit \}$$
One possible event is $A = \{ \heartsuit , \diamondsuit\}$.
Another possible event is $B = \{ \diamondsuit , \clubsuit, \spadesuit \}$. The
union (unioni) of the two events includes all elements of both
$$A \cup B = \{ \heartsuit , \diamondsuit , \clubsuit, \spadesuit \}$$
The intersection (leikkaus) includes elements that are common to both events
$$A \cap B = \{ \diamondsuit \}$$
The complement (komplementti) of a set includes al elements of $S$ that are not
included in $A$
$$A^c = \{ \clubsuit, \spadesuit \}$$
Events $A$ and $B$ are said to be disjoint (erillisi\"a), if 
$$A \cap B = \emptyset$$
Number of events $A_1$, $A_2$, $A_3$, \ldots are said to be pairwise disjoint,
if
$$A_i \cap A_j = \emptyset$$
for all pairs of i, j. In addition, if $\bigcup_{i=1}^{\infty}A_i = S$,
then $A_1$, $A_2$, $A_3$, \ldots defines a partition of the sample space.
\end{example}
\begin{example}
Events $A = \{ \heartsuit , \diamondsuit\}$ and $B = \{\clubsuit, \spadesuit
\}$ are disjoint since 
$$A \cap B = \emptyset$$
Events $A_1=\{\heartsuit,
\diamondsuit \}$, $A_2 = \{ \clubsuit \}$ and $A_3 = \{ \spadesuit \}$
are pairwise disjoint. Also, since $$\bigcup_{i=1}^{3} = A_1 \cup A_2 \cup A_3 =
\{\heartsuit , \diamondsuit , \clubsuit, \spadesuit\} = S$$
they are also partition of S.
\end{example}
% TODO: Do we something about the dice example? In pdf notes, just example of
% sample space of dice throw experiment, and how it is numerated. {1,2,3,4,5,6}
\begin{definition}
Probability (todennäköisyys)

If a certain experiment is performed number of times (or infinite number of
times), it may lead to different outcome, which is an event of the sample space.
This frequency of outcome of an event is called probability.

For an event $A \subset S$ in an experiment, notation $P(A)$ (or $Pr(A)$)
specifies the probability of outcome / event $A$.
\end{definition}
\begin{theorem}
Axioms of probability
\begin{itemize}
  \item[1.]For every event $A$, $P(A) \neq 0$ (meaning every event is possible)
  \item[2.]$P(S) = 1$ (because something will be observed)
  \item[3.]For a sequence of pairwise disjoint
  events,$$P(\bigcup_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty}P(A_i)$$
\end{itemize}
\end{theorem}
\begin{example}
%TODO: There were some diagrams related to these examples, that are not yet
% added. Should they be?
Assume we have two events $A_1$, $X \in A_1$ and $A_2$, $X \in A_2$, which have
probabilities $P(A_1) = 0.2$ and $P(A_2) = 0.3$.

%\textbf{Case 1}%
If the events are disjoint ($A_1 \cap A_2 = \emptyset$),
% then
probability for the union of events is $$P(A_1 \cup A_2) = P(A_1) + P(A_2) = 0.2
+ 0.3 = 0.5$$

%\textbf{Case 2}%
If events are not disjoint ($A_1 \cap A_2 \neq \emptyset$), then
$$P(A_1 \cup A_2) \neq P(A_1) + P(A_2) = 0.2 + 0.3 = 0.5$$
\end{example}
\begin{example}
In a fair deck, define events $$A_1 = \{\heartsuit\}, A_2 = \{\diamondsuit\},
A_3 = \{\clubsuit\}, A_4 = \{\spadesuit\}$$which have probabilities
$$P(A_1) = P(A_2) = P(A_3) = P(A_4) = 1/4$$
Events $A_1 \ldots A_4$ are disjoint. Therefor,
$$B = A_1 \cup A_2 = \{\heartsuit, \diamondsuit\}$$
$$P(B) = P(A_1 \cup A_2) = P(A_1) + P(A_2) = 1/4 + 1/4 = 1/2$$
\end{example}
\begin{theorem}
Consider events A and B
\begin{itemize}
  \item[1.]$P(A^c) = 1 - P(A)$
  \item[2.]$P(B \cap A^c) = P(B) - P(A \cap B)$
  \item[3.]$P(A \cup B) = P(A) + P(B) - P(A \cap B)$
  \item[4.]If $A \subset B$, then $P(A) \leq P(B)$
\end{itemize}
\textbf{Note} Consider case 3 of theorem \ref{Theorem2}
$$P(A \cup B) = P(A) + P(B)-P(A \cap B)$$
$$P(A \cup B) \leq 1$$
$$P(A) + P(B)-P(A \cap B) \leq 1$$
$$P(A \cap B) \geq P(A) + P(B)-1$$
This equation is called the Bonferroni inequality. Idea is, that if we have
intersection of two events ($A$ and $B$), the probability of the intersection
can be shown to be higher or equal than the right term.

$$P(A \cap B) \geq P(A) + P(B)-1$$
Supose $A$ and $B$ are two events that occur with probability $P(A) = P(B) =
0.975$. Then $P(A \cap B) = 0.975 + 0.975 - 1 = 0.95\ldots$ 


\label{Theorem2}
\end{theorem}
\begin{theorem}
\begin{itemize}
  \item[a)] $P(A) = \sum_{i=1}^{\infty} P(A \cap C)$ for any partition
  $C_1, C_2, \ldots$
  \item[b)]$P(\bigcup_{i=1}^{\infty}A_i) \leq \sum_{i=1}^{\infty}P(A_i)$, for
  any sets $A_1, A_2, \ldots$
\end{itemize}
\end{theorem}
\begin{definition}
IF $A$ and $B$ are events in a sample space $S$ and $P(B) > 0$, then
conditional probability (ehdollinen todenn\"akoisyys) of $A$ given $B$ is
$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$
\end{definition}
\begin{example}
\label{clinicalExample1}
Clinical trial. Assume  that we know the probabilities that are represented in
table \ref{probTable1}.
Let event $A$ be the event ``the patient recovered'' with sample space $S =
\{ ``OK", ``NOT~OK"\}$, and event B is the event ``The patient was treated
with placebo'' with $S = \{``YES'', ``NO''\}$.

Now $P(A \cap B)$ can be computed directly using the table \ref{probTable1}, and
it is
$$P(A \mid B) = P(``OK'' \mid ``Placebo'') = \frac{P(A \cap B)}{P(B)} =
\frac{0.160}{0.227} \approx 0.70$$

\begin{table}
\caption{Probabilities related to clinical trial. Four different drugs and the
probabilities that patient got or did not got ok after using it.}
\label{probTable1}
\begin{tabular}{| r | r | r | r | r | r |}
\hline
{ } & Drug1 & Drug2 & Drug3 & Placebo & Total \\
\hline
OK & 0.120 & 0.087 & 0.147 & 0.160 & 0.513 \\
\hline
{NOT OK} & 0.147 & 0.167 & 0.107 & 0.067 & 0.487 \\
\hline
Total & 0.267 & 0.253 & 0.253 & 0.227 & 1.000 \\
\hline
\end{tabular}
\end{table}
\end{example}
\begin{definition}
%TODO: Can equations here be done more better?
Bayes rule

Assuming we have event $A$ and $B$,
$$P(A|B) = \frac{P(A \cap B)}{P(B)} ~~\|* P(B)$$
if $P(B) > 0$
$$P(A \cap B) = P(B)P(A|B)$$

And the other way around
$$P(B|A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A \cap B)}{P(A)} ~~\|* P(A)$$
if $P(A) > 0$
$$P(A \cap B) = P(A)P(B|A)$$
and by using the earlier $P(A \cap B) = P(B)P(A|B)$
$$P(B)P(A|B) = P(A)P(B|A) ~~\|: P(B)$$
$$P(A|B) = \frac{P(A)P(B|A)}{P(B)}$$
This is known as Bayes rule.
\end{definition}
\begin{definition}
Two events $A$ and $B$ are independent, if
$$ P(A \cap B) = P(A)P(B)$$
If two events are known to be independent, we can compute $P(A \cap B)$ as
$P(A)P(B)$
\end{definition}
\begin{example}
In example \ref{clinicalExample1} we had $P(A) = 0.513$, $P(B)=0.227$ and
$P(A \cap B) = 0.160$. By calculating
$$P(A)P(B) = 0.513 * 0.227 = 0.116 \neq 0.160 = P(A \cap B)$$
we can determine that events $A$ and $B$ are not independent.
\end{example}
\chapter{Random variables}
\section{Random variables}
\begin{definition}
A random variable (RV for short) (satunnaismuuttuja) is a function from sample
space S into the real numbers.\\
\textbf{Note} If S includes only real numbers, the definition of the RV is
implicit. Some examples of this in table \ref{tableImplicitEventRV1}. Examples
of events that not implicitly connected to random variables in table
\ref{tableImplicitEventRV2}
\newline
%TODO: Can we make the aligment of table cells better?
\begin{table}
\caption{Examples of implicit connection between event and random variable}
\label{tableImplicitEventRV1}
\begin{centering}
\begin{tabular}{p{6cm} | p{6cm}}
\textbf{Experiment} & \textbf{Event = Random variable} \\ \hline
Toss of two dice. & $X$ = Sum of two numbers \\ \hline
Estimated regression from n observations from certain true model.
& $X$ = Estimate of slope coefficient $\hat{\beta}_2$ \\ \hline
Event location in space. & $X$ = \# of events in certain fixed subset of the
space.
\end{tabular}
\end{centering}
\end{table}

\begin{table}
\caption{Examples of implicit connection between event and random variable}
\label{tableImplicitEventRV2}
\begin{centering}
\begin{tabular}{p{3cm} | p{5cm} | p{4cm}}
\textbf{Experiment} & \textbf{S} & \textbf{RV} \\ \hline
Toss of a coin & $\{``H'', ``T''\}$ & \begin{equation*}
X_1 = \left\{
\begin{array}{rl}
1 & \text{if Head}, \\
0 & \text{if Tails}
\end{array} \right.
\end{equation*}
or
\begin{equation*}
X_2 = \left\{
\begin{array}{rl}
10 & \text{if Head}, \\
2 & \text{if Tails}
\end{array} \right.
\end{equation*}
\\ \hline
Health status of tree & $\{``Healthy", ``Sick", ``Dead"\}$ & \begin{equation*}
X = \left\{
\begin{array}{rl}
1 & \text{if Healthy}, \\
2 & \text{if Sick}, \\
3 & \text{if Dead}
\end{array} \right.
\end{equation*}
or we can define two random variables
\begin{equation*}
X_1 = \left\{
\begin{array}{rl}
0 & \text{if Healthy or Dead}, \\
1 & \text{if Sick}
\end{array} \right.
\end{equation*}
\begin{equation*}
X_2 = \left\{
\begin{array}{rl}
0 & \text{if Healthy or Sick}, \\
1 & \text{if Dead}
\end{array} \right.
\end{equation*}
\\
\end{tabular}
\end{centering}
\end{table}
The probability function of the original sample space is defined ot the RV as
follows:

Consider the sample space $S = {S_1, S_2, \ldots ,S_n}$, and has probability
function P. Define RV $X$ with sample space / range / support set
$$ \mathcal{X} = \{x_1, x_2, \ldots , x_n \}$$ 
% TODO: Is mathcal font good to use with RV supports?.
$$P_X(X = x_i) = P(\{ s_j \in S : X(s_j) = x_i \})$$
\textbf{Note} About the notations: Uppercase (capital) letters are used
for a random variables and lowercase letters for the realized value or range. 
$X = x$ means, that RV $X$ gets value $x$.
\end{definition}
\newpage
\begin{example}
\label{exampleThreeCoinToss}
Tree tosses of a coin. Let RV $X$ be the \# of heads, or RV $Y$ is the number of
tails. For RV $X$, well get space $$\mathcal{X} = \{0,1,2,3\}$$ See table
\ref{exampleThreeCoinTossTable1} to see how the sample space values are
converted to random variable values.
We can then calculate the probabilities that random variable gets certain value
from its support space. For random variable $X$, these probabilities are
%TODO: Better way to add space between tabular and text?
\newline
\newline
\begin{tabular}{r | r r r r}
$x$ & 0 & 1 & 2 & 3 \\
$P(X=x)$ & $\frac{1}{8}$ & $\frac{3}{8}$ & $\frac{3}{8}$ & $\frac{1}{8}$
\end{tabular}
\newline
\newline
Now, we can get the probability of event, that with three coin tosses,
we get only one heads, by getting the probability that random variable $X$
receives value 1 $$P(X=1) = P(\{HTT,~THT,~TTH\}) = \frac{1}{8}+\frac{1}{8}+\frac{1}{8} =
\frac{3}{8}$$
\end{example}
\begin{table}
\caption{Example \ref{exampleThreeCoinToss} experiment's sample
space value conversion to RV values.}
\label{exampleThreeCoinTossTable1}
\begin{tabular}{ r | r  r  r  r  r  r  r  r }
$s_i$ & HHH & HHT & HTH & HTT & THH & THT & TTH & TTT \\
\hline
$X(s_i)$ & 3 & 2 & 2 & 1 & 2 & 1 & 1 & 0 \\
$Y(s_i)$ & 0 & 1 & 1 & 2 & 2 & 1 & 2 & 3 \\
\hline
\end{tabular}
\end{table}
\begin{example}
Waiting time in a phone service. Sample space for experiment is
$$S = [0,60)$$
For RV $X$, space will be
$$\mathcal{X} = [0, 60)$$
meaning since the original experiment has sample space that is defined by real
number, the experiment directly specifies the random variable
 $$X(s) = s$$
\end{example}
\section{Cumulative distribution function}
\begin{itemize}
  \item Starting point with any computation of probability is the distribution
  function.
\end{itemize}
\begin{definition}
The culmulative distribution function (kertym\"afunktio) or just distribution
function (cdf for short) is defined as
\begin{equation*}
F_X(x) = P_X(X \leq x)~\text{for all $x$}
\end{equation*}
\end{definition}
\begin{example}
Tossing of 3 coins. Random variable $X$ = the \# of heads. Cumulative
distribution function for RV $X$ can be defined as
\begin{equation*}
F_X(x) = \left\{
\begin{array}{rll}
0 & \text{if } -\infty < x < 0 & x \in (-\infty, 0) \\
\frac{1}{8} & \text{if } 0 \leq x < 0 & x \in [0, 1) \\
\frac{1}{2} & \text{if } 1 \leq x < 0 & x \in [1, 2) \\
\frac{7}{8} & \text{if } 2 \leq x < 0 & x \in [2, 3) \\
1 & \text{if } 3 \leq x < \infty & x \in [3, \infty) 
\end{array} \right.
\end{equation*}
%TODO: There is a plot about the cdf and also some specification how value for
% 1/2 and 7/8 are gotten. Are they needed?
\textbf{Notes}
\begin{itemize}
  \item[-] $F_X(x)$ is defined for all $x$!
  %TODO: If the plot for cdf is added, here is additional note.
  %\item[-] $F_X(x) has jumps at x_i \in \chi$. The jumps equal to P(X=x_i).
  \item[-] $F_X(x) = 0$ for $x < 0$ and $F_X(x) = 1$ for $x \geq 3$.
  \item[-] $F_X(x)$ is right-continuous: the value of $F_X(x)$ at the jump is
  the one we get by approaching the jump from the right.
  \item[-] $F_X(x)$ is discontinuous (in this example).
\end{itemize}
\begin{theorem}
\label{rulesForCdfTheorem}
Function $F(x)$ is a cdf if and only if the following conditions hold:
\begin{itemize}
  \item[a)] $\lim_{x \rightarrow -\infty}F(x) = 0$ and $\lim_{x \rightarrow
  \infty}F(x) = 1$.
  \item[b)] $F(x)$ is non-decreasing (kasvava) (meaning value of function
  is either increasing or constant as $x$ increases)
  \item[c)] $F(x)$ is right-continuous, i.e. for every $x_0$ 
  $$\lim_{x \downarrow x_0}F(x) = F(x_0)$$
\end{itemize}
\end{theorem}
\begin{itemize}
  \item If a function is said to be a cdf, it has to fulfill the rules a, b and
  c of theorem \ref{rulesForCdfTheorem}.
  \item If a function fulfills rules a, b and c of theorem
  \ref{rulesForCdfTheorem}, then it can be used as a cdf.
\end{itemize}
\begin{theorem}
A random variable $X$ is said to be continuous, if $F_X(x)$ is a continuous
function of x (meaning the function foes hot have jumps).
\end{theorem}
\textbf{Note} The $X$ in example \ref{exampleThreeCoinToss} was discrete.
\end{example}
\begin{example}
\label{exampleUniformDistribution1}
An example of a continuous cdf is the uniform distribution. Let RV $U$ have
equal probabilities within $\mathcal{U} \in [a, b]$. The cdf of $U$ is
\begin{equation*}
F_U(u) = \left\{
\begin{array}{rll}
0 & \text{if } u < a & u \in (-\infty, a) \\
\frac{u-a}{b-a} & \text{if }  a \leq u < b & u \in [a, b) \\
1 & \text{if } u \geq b & u \in [b, \infty) 
\end{array} \right.
\end{equation*}
%TODO: Should we add the plot for the cdf here?
\end{example}
\begin{itemize}
  \item The cdf can be used for calculation of the probabilities. In general
  $$P(x \in (l, u]) = F(u) - F(l)$$
\end{itemize}
\begin{example}
Consider the tossing of 3 coins (example \ref{exampleThreeCoinToss}).
Probability that more than 1 heads is observed is $$P(X \in (1,3]) =
F_X(3)-F_X(1) = 1 - 0.5 = 0.5$$
This result is equal to
$$P(X=2)+P(X=3)=\frac{3}{8}+\frac{1}{8} = \frac{4}{8} = \frac{1}{2}$$
\end{example}
\begin{example}
Consider uniform distribution where $a=0$ and $b=2$.
$$P(X \in (0.5, 1]) = F(1) - F(0.5) = 0.5 - 0.25 = 0.25$$
\end{example}
\textbf{Note} For continuous RV %TODO: Should this be elaborated more?
$$P(x \in [a,b]) = P(x \in (a,b)) = P(x \in (a, b]) = P(x \in [a, b))$$
\begin{definition}
RVs $X$ and $Y$ are said to be identically distributed (samoin jakautuneita), if
$F_X(x) = F_Y(x)$ for every x.
%TODO: Do we need plot here?

Two RVs are said to be independent and identically distributed (i.i.d.) if they
are both independent and identically distributed.
\end{definition}
\section{Probability mass and density functions}
\begin{itemize}
  \item The probability mass function (pmf) is defined only for discrete RVs as follows.
$$f_X(x) = P(X=x)~\text{for all }x$$
\item The pmf is connected with cdf as follows.
$$F_X(x) = \sum_{k=1}^{X}f_X(k)$$
\end{itemize}
\begin{example}
\label{exampleBernoulliDistribution1}
The $Bernoulli(p)$ distribution is used for a discrete RV with two possible
values (i.e. it is the distribution for so called Bernoulli trial, which has
two possible outcomes: success (``S'') and failure (``F'')).
Record the outcomes as a RV:

\begin{tabular}{ l | l }
$s_i$ & $X(s_i)$ \\ \hline
``S'' & 1 \\
``F'' & 0
\end{tabular}

The pmf of $X$ is
\begin{equation*}
F(x; p) = \left\{
\begin{array}{ll}
1-p & x = 0 \\
p & x = 1 \\
0 & \text{elsewhere} 
\end{array} \right.
\end{equation*}
The Bernoulli cdf is
\begin{equation*}
F(x; p) = \left\{
\begin{array}{ll}
0 & x < 0 \\
1-p & 0 \leq x < 1 \\
1 & x \geq 1 %TODO: Is this correct? In notes was \geq 0?
\end{array} \right.
\end{equation*}
\textbf{Note} $a$ and $b$ for the uniform distribution (like in example
\ref{exampleUniformDistribution1}) and the $p$ in case of Bernoulli distribution
(like in example \ref{exampleBernoulliDistribution1}) are called parameters.
\end{example}
%TODO: Is the plot needed?
\begin{itemize}
  \item pmf cannot be defined for a continuous RV since it holds
  $$P(X=x) \leq \lim_{\epsilon \downarrow 0} [F_X(x) - F_X(x - \epsilon)] = 0$$
  \item For continuous RV, the sum of pmf is replaced by integral\ldots
  $$P(X \leq x) = F_X(x) = \int_{-\infty}^{x} f_X(t)dt$$
  \item \ldots which also implies
  $$\frac{d}{\ud x}F_X(x)=f_X(x)$$
\end{itemize}
\begin{definition}
Function $f_X(x)$ above is called probability density function (pdf for short)
(tiheysfunktio).
\end{definition}
\begin{theorem}
A function $f(x)$ is a pdf or pmf, if
\begin{itemize}
  \item[a)]$f(x) \geq 0 $ for all x
  \item[b)]$$\sum_X f(x) = 1~\text{(if pmf)}$$
  or
  $$\int_{-\infty}^{\infty} f(x) \ud x = 1~\text{(if pdf)}$$
\end{itemize}
%TODO: Are the plots for pmf and pdf needed here?
\end{theorem}
\textbf{Note} Notation $\sim$ means ``is distributed as''; for example
\begin{itemize}
  \item $X \sim f(x)$
  \item $X \sim F(x)$
  \item $X \sim Bernoulli(p)$
\end{itemize}
\begin{example}
Perhaps the most widely used continuous distribution is the Normal distribution,
which has support $(-\infty, \infty)$, and pdf
$$f(x; \mu, \sigma) =
\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},~-\infty < \mu <
\infty,~\sigma > 0$$
If $\mu = 0$ and $\sigma = 1$, then function is called standard normal density
(standardoitu normaali jakauma).
%TODO: Is the plot here needed?
The cdf of normal distribution 
$$\int_{-\infty}^{x}f_X(t)dt =
\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{x}e^{-\frac{(t-\mu)^2}{\sigma^2}}dt=\ldots$$
%TODO: is the \sigma^2 correct above or is it just \sigma?
but it cannot be written in closed form. However, R and other software can
compute the value numerically.

\textbf{Note} Parameters: $\mu$ is the expected value, and $\sigma$ is the
standard deviation of X.
\end{example}
\begin{example}
Let $U \sim Uniform(0, 10)$ and X be the sample mean
$\frac{\sum_{i=1}^{n}u_i}{n}$ i n a sample of size n. R-script \verb#unifmean.R#
illustrates, that if $n$ is sufficiently large, then
$$X \sim N(\frac{10}{2}, \frac{10/\sqrt{12}}{n})$$
This is a consequence of central limit theorem.
\end{example}
%TODO: Is the bernoulli example needed (multiple left/right bernoulli
% experiments from normal distribution)? Leaving it out for now, since relies so much in the
% visualization.
\textbf{Note} Often the parameters of a distribution are functions, not singe
numbers. For example, the probability of success, $p$, in the Bernoulli case
may depend on some fixed/known characteristics $x$ of sampling  unit, ie. we
have $p(x)$ instead of the p. Or the mean of normally distributed RV may also
depend on $x$, so we use $\mu(x)$ instead of $\mu$. Examples 1.1. and 1.2. in
\verb#notes.pdf# demonstrate this
\begin{example} $p(Age) = 0.1 + 0.0005*Age$
\begin{itemize}
  \item [If] $Age = 0$, then $p(0) = 0.1$.
  \item [If] $Age = 10$, then $p(10) = 0.15$
  \item [If] $Age = 100$, then $p(10) = 0.1 + 0.5 = 0.6$
\end{itemize}
Since $p(Age)$ as specified above may become negative or higher than 1 with some
values of the parameter $Age$, function $p(x)$ should be defined so, that it
can only get values within $(0,1)$. This could be achieved with binary logistic
model.
\end{example}
%TODO: Should we have these two examples (one above and one bellow)?
%\begin{example} We have normal distributed random variable $Y \sim N(\mu(x),
%\sigma)$, where
%$\mu(x) = 0.5*x$, where $x$ is tree age and $\sigma = 2$. $Y$ specifies the
%diameter of the tree.
%
%\end{example}
\section{Transformations of a random variable}
\begin{itemize}
  \item If $X$ is a random variable, then any function of it, e.g. $Y=g(x):
  \mathcal{X} \rightarrow \mathcal{Y}$ is also a random variable, where the
  function $g(x)$ is called transformation.
  \item The probabilistic properties of Y can be expressed using those of X:
  $$P(Y \in A) = P(g(x) \in A)$$
  \item If $g(x)$ is monotonic function in $X$, we can define the inverse
  function $g^{-1}(y): \mathcal{Y} \rightarrow \mathcal{X}$ and establish the
  theorem \ref{theoremTransformations1}.
\end{itemize}
\begin{theorem}
\label{theoremTransformations1}
Let $X$ have cdf $F_X(x)$. Let $Y = g(x)$ and $\mathcal{X} = \{x, f_X(x) > 0\}$
and $\mathcal{Y} = \{y: y=(g(x)~\text{for some } x \in \mathcal{X}\}$
\begin{itemize}
  \item[a)] If $g$ is an increasing function of $\mathcal{X}$, then 
  $$F_Y(y) = F_X(g^{-1}(y))\text{ for }y \in \mathcal{Y}$$
  \begin{itemize}
    \item If $g(x)$ specifies $y(x)$, we solve $y(x) = y$ for $x$ to write $x$
    in terms of $y$ $x(y) = g^{-1}(y)$.
  \end{itemize}
  \item[b)] If $g$ is decreasing function of $\mathcal{X}$ and $X$ is
  continuous, then 
  $$F_Y(y) = 1 - F_X(g^{-1}(y))$$
\end{itemize}
\end{theorem}
\begin{example}
Suppose $X \sim Uniform(0, 1)$. Now $F_X(x) = x,~ 0 < x < 1$. Consider
transformation $Y=g(x) = -ln(x)$.
\begin{itemize}
  \item $ln(x)$ is decreasing, since $\frac{\ud }{\ud x}(-ln(x)) = -\frac{1}{x} <
  0$ when $x > 0$
  $$\mathcal{X} \in (0,1) \rightarrow \mathcal{Y} \in (0, \infty)$$
\item Because $g(x)$ is decreasing, the minimum of $Y$ will be obtained at the
maximum value of $X$.
$$-ln(1) = 0,~\text{(minimum of $Y$)}$$
\item The maximum of $Y$ is obtained at the minimum of $X$.
$$-ln(x) \rightarrow \infty,~\text{as } x \rightarrow \infty $$
\item We can get the $g^{-1}$ by solving $x$ for $y = g(x)$
$$y = -ln(x)~~||*(-1)$$
$$-y = ln(x)$$
$$x = e^{-y} = g^{-1}(y)$$
\item The cdf of Y is
$$F_Y(y) = 1 - F_X(g^{-1}(y)) = 1 - F_X(e^{-y}) = 1 - e^{-y}$$
\item The pdf of Y is
$$f_Y(y) = \frac{\ud F_Y(y)}{\ud y} = \frac{\ud (1-e^{-y})}{\ud y} = -e^{-y}(-1)=e^{-y}$$
%TODO: In notes, it was exponential pdf, but capital F is suposed to mean cdf.
\item Note: $F_Y(y)$ is called the exponential cdf with the rate parameter 1.
$f_Y(y)$ is the corresponding density.
\end{itemize}
%TODO: Two plots on page 2 of notes 5. Should they be added?
\end{example}
\begin{theorem} Let $X \sim f_X(x)$ and $Y=g(X)$ be a monotone transformation,
and supports $\mathcal{X} = \{x, f_X(x) > 0\}$
and $\mathcal{Y} = \{y: y=(g(x)~\text{for some } x \in \mathcal{X}\}$ (like in
theorem \ref{theoremTransformations1}). Suppose that $f_X(x)$ is continuous on
$\mathcal{X}$ and $g^{-1}(y)$ has continuous derivative on $\mathcal Y$.

The pdf of $Y$ is
\begin{equation*} f_Y(y) =
\left\{
\begin{array}{ll}
f_X(g^{-1}(y)) \vert \frac{\ud }{\ud x}g^{-1}(y) \vert & y \in \mathcal{Y} \\
0 & otherwise
\end{array} \right.
\end{equation*}
\end{theorem}
%TODO: Should we have the example here (that just tells to look the previous)?
\begin{definition}
Let $X$ have cdf $F(x)$. Quartile function of $X$ is defined as the inverse of
cdf:
$$q(u) = F^{-1}(u)$$
\end{definition}
\begin{example}
We want to find a value of $X$ such that $P(X \leq x) = 0.95$. The solution is
given by $q(x) = q(0.95) = \ldots = x_*$ %TODO: Is this correct?
\end{example}
\begin{example}
Random number generator. Let $q_x(u)$  be the quartile function of $X$. If we
are able to generate a sample from $U \sim Uniform(0,1)$ distribution,
transformation $g(u) = q_X(u)$ provides a random sample from the distribution of $X$.
\begin{equation*} F(x) =
\left\{
\begin{array}{lll}
0 & x \leq 0 & \\ %TODO: In the notes was x \leq x but used x \leq 0. Correct?
0.1x & \text{when }0 < x \leq 5 & (*) \\
0.25 + 0.05x & \text{when }5 < x \leq 15 & (**) \\
1 & x > 15
\end{array}\right.
\end{equation*}
Then, the quartile functions for parts (*) and (**) are solved from $F(x) = u$
by solving x. 

Part (*)
$$0.1x = u$$
$$x = 10u$$
$$\text{ for } F(0) < u \leq F(5) \rightarrow 0 < u \leq 0.5$$

Part (**)
$$0.25+0.05x = u$$
$$x = 20u - 5$$
$$\text{ for } F(5) < u \leq F(15) \rightarrow 0.5 < u \leq 1$$
Which provide us with the quartile function for $X$
\begin{equation*} q(u) =
\left\{
\begin{array}{lll}
10u & \text{if }u \in [0, 0.5) \\
20u-5 + 0.05x & \text{if }u \in [0.5, 1] \\
\end{array} \right.
\end{equation*}
Quartile function $q(u)$ implemented in R-script \verb#probIntTrans.R#.
%TODO: Is the plot from page 4 of notes 6 needed?
\end{example}
\section{Expected value}
\begin{definition}
Expected value or mean (odotusarvo) of a random variable $g(X)$ is
\begin{equation*} E(g(X)) =
\left\{
\begin{array}{ll}
\int_{-\infty}^{\infty} g(x)f(x) \ud x & \text{if $x$ is continuous} \\
\sum_{x \in \mathcal{X}} g(x)f(x)& \text{if $x$ is discrete}
\end{array} \right.
\end{equation*}
provided that sum/integral exists. If $E(g(X)) = \infty$, we say that the
expected value does not exist.

\textbf{Note} If $g(x) = x$, the above rule simplifies to
\begin{equation*} E(g(X)) =
\left\{
\begin{array}{ll}
\int_{-\infty}^{\infty} xf(x) \ud x & \text{if $x$ is continuous} \\
\sum_{x \in \mathcal{X}} xf(x) & \text{if $x$ is discrete}
\end{array} \right.
\end{equation*}

\end{definition}
\begin{example}
Let $X \sim Exponential(\lambda)$ which has the pdf
$$\begin{array}{ll}
f_X(x) = \frac{1}{\lambda}e^{-\frac{x}{\lambda}} & 0 < x < \infty
\end{array}$$
And the expected value is
%TODO: There are many explanations in the original notes how different stages of
% integration are achived, but left them out for now.
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
E(X) & = \int_{0}^{\infty} xf_X(x) \ud x \\
& =-\int_{0}^{\infty}x\left (-\frac{1}{\lambda}e^{-\frac{x}{\lambda}}\right )\ud x
\\
& = \Big|_{0}^{\infty}
-xe^{-\frac{x}{\lambda}}+\int_{0}^{\infty}1e^{-\frac{x}{\lambda}}\ud x \\
& = -0-(-0)+\left (
-\lambda\int_{0}^{\infty}-\frac{1}{\lambda}e^{-\frac{x}{\lambda}}\ud x \right ) \\
& =-\lambda \Big|_{0}^{\infty} e^{-\frac{x}{\lambda}} \\
& = -\lambda(0-1) = \lambda
\end{array}
\end{equation*}
%E(X) = \int_{0}^{\infty} xf_X(x) dx
%=-\int_{0}^{\infty}x\left (-\frac{1}{\lambda}e^{-\frac{x}{\lambda}}\right )dx
% \\
%= \Big|_{0}^{\infty}
%-xe^{-\frac{x}{\lambda}}+\int_{0}^{\infty}1e^{-\frac{x}{\lambda}}dx =
%-0-(-0)+\left (
%-\lambda\int_{0}^{\infty}-\frac{1}{\lambda}e^{-\frac{x}{\lambda}}dx \right ) \\
%=-\lambda \Big|_{0}^{\infty} e^{-\frac{x}{\lambda}} = -\lambda(0-1) = \lambda
Computation of the expected values can get quite hard. Examples 1.10. and 1.11.
in \verb#notes.pdf# demonstrate numerical approximations.
%TODO: There we some additional stuff about using R in the integration. Needed?
\end{example}
%TODO: Are the notes about how integration is done in R needed? Page 1 notes 7.
\begin{theorem}
Let $X$ and $Y$ be random variables and $a$, $b$ and $c$ fixed constants. The
following rules hold:
\begin{itemize}
\item[] $E(c) = c$
\item[] $E(cX) = cE(X)$
\item[] $E(X+Y) = E(X) + E(Y)$
\item[] $E(X+c) = E(X) + c$
\item[] $E(XY) \neq E(X)E(Y)$
\begin{itemize}
  \item $E(XY)$ depends on distribution of $X$ and $Y$.
\end{itemize}
\item[] $E(g(X)) \neq g(E(X))$
\begin{itemize}
  \item If $g$ is nonlinear function of X.
  \item Also depends on the distribution of X.
  \item If $g$ is linear, then we get rule (*).
\end{itemize}
\textbf{Also}
\item[] $E(a*g_1(X) + b*g_2(Y)+c) = a*E(g_1(X))+b*E(g_2(Y))+c$
\end{itemize}
\end{theorem}
\textbf{Note} $E(Y) = E(g(X))$ can be computed in two ways:
\begin{itemize}
  \item[1)] $E(g(x)) = \int_{-\infty}^{\infty}g(x)f_X(x)\ud x$
  \item[2)] $E(g(x)) = E(Y) = \int_{-\infty}^{\infty}yf_Y(y)\ud y =
  \int_{-\infty}^{\infty}yf_X(g^{-1}(y)) \left|\left| \frac{\ud }{\ud y}g^{-1}(y)
  \right|\right| \ud y$ %TODO: Better way to do double lines?
\end{itemize}
\begin{definition}
The variance is defined as
$$Var(X) = E\left[\left(X-E(X)\right)^2\right],\text{ and } var(X) \geq 0\text{
always.}$$
The square root of $Var(X)$ is called the standard deviation sd
$$sd(x) = \sqrt{Var(X)}$$
\end{definition}
\begin{example}
Let $X \sim Exponential(\lambda)$, meaning the $X$ has pdf $f_X(x) =
\frac{1}{\lambda}e^{-\frac{x}{\lambda}}$. The variance of $X$ is
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
Var(X) & = E\left[(x-\lambda)^2\right] \\
& = E(g(X)) \\
& =
\int_{-\infty}^{\infty}(x-\lambda)^2\frac{1}{\lambda}e^{-\frac{x}{\lambda}}\ud x \\
&=
\int_{-\infty}^{\infty}(x^2-2x\lambda+\lambda^2)\frac{1}{\lambda}e^{-\frac{x}{\lambda}}\ud x
\\
&= \int_{-\infty}^{\infty}x^2\frac{1}{\lambda}e^{-\frac{x}{\lambda}}\ud x -
\int_{-\infty}^{\infty}2x\lambda \frac{1}{\lambda}e^{-\frac{x}{\lambda}}\ud x+
\int_{-\infty}^{\infty}\lambda^2 \frac{1}{\lambda}e^{-\frac{x}{\lambda}}\ud x \\
&=\text{(integrate each term by parts)} = \lambda^2
\end{array}
\end{equation*}
\end{example}
\textbf{Note} A computationally convenient form of $Var(x)$ is obtained as
follows:
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
Var(X)& = E\left[(X-E(X))^2\right] \\
& = E\left[X^2 - 2XE(X) + [E(X)]^2 \right] \\
& = E(X^2) - E[2XE(X)] + E[(E(X))^2] \\
& = E(X^2) - 2[E(X)]^2+ [E(X)]^2 \\
& = E(X^2) - [E(X)]^2
\end{array}
\end{equation*}
So $Var(X) = E\left[(X-E(X))^2\right] = E(X^2) - [E(X)]^2$. For continuous
random variables, the variance can be approximated numerically using integrate.
See example 1.4. in \verb#notes.pdf#. We can either use
$$Var(X)=\int_{-\infty}^{\infty}[X-E(X)]^2f(x)\ud x$$
or
$$Var(X) = E(X^2)-[E(X)]^2 = \int_{-\infty}^{\infty}x2f(x)\ud x 
- \left(\int_{-\infty}^{\infty}xf(x)\ud x \right )^2$$
\begin{theorem}
Let $X$ be a random variable with a finite variance and $a$ and $b$ are finite
constants. Then
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}
Var(aX+b)& = a^2Var(X) \\
sd(aX+b) & = \sqrt{Var(aX+b)} \\
& = \sqrt{a^2Var(X)} & = a\sqrt{Var(X)} \\
& & =a*sd(x)
\end{array}
\end{equation*}
\end{theorem}
\begin{definition}
In general, the expected value $\mu_n^{'} = E(X^n)$ is called
n\textsuperscript{th} moment of random variable $X$. The expected value $\mu_n =
E(X -\mu)^n$ is called the n\textsuperscript{th} central moment.

Expected value of $X$ is the 1\textsuperscript{st} moment and the variance of $X$ is the 2\textsuperscript{nd}
moment of $X$. These two are called also the 1\textsuperscript{st} and
2\textsuperscript{nd} order properties of the (univariate) random variable X.
  $$
  \begin{array}{ll}
  E(X)=\mu_{1}^{´}, & Var(X) = \mu_2 = \mu_{2}^{´} - (\mu_{1}^{´})^2
  \end{array}
  $$
1\textsuperscript{st} moment specifies the location of the distribution. The
2\textsuperscript{nd} moment is related to the width of the distribution. The
higher moments specify the shape of the distribution, eg. 3\textsuperscript{rd}
moment is related to the skewness of the distribution, and the
4\textsuperscript{th} on the centroids.

The moments are ``in the order of importance'': expected value is often the most
important single number we can specify for the random variable $X$; the
variance is the second most important etc.
\end{definition}

%new chapter, starts from notes 8, page 2
\chapter{Bivariate random variables}
\begin{definition}
Let $X$ and $Y$ be two discrete random variables. The joint pmf is defined as
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{lll}
&f(x,y) = P(X=x,Y=y) &\text{ for all } x,y. \\
\text{or} & f_{X,Y}(x,y) = P(X=x, Y=y) & f:\ureal^2 \rightarrow \ureal
\end{array}
\end{equation*}
\end{definition}
\begin{example}
\label{exampleTwoDice1}
Two dice.
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{l}
X = \text{The sum of two dice} \\
Y = \left | \text{The difference of two dice} \right |
\end{array}
\end{equation*}
The sample space includes $\left\{ (1,1), (1,2), (1,3),\ldots,(6,6) \right\}$.
There are total of 36 different possibilities, and they are are equally likely
with probability $\frac{1}{36}$.

The joint pmf of $(X,Y)$ can be expressed in a 2x2 table
\ref{exampleTwoDice1Table1}
\begin{table}[h]
\caption{Probabilities of joint pmf $f_{X,Y}(x,y)$ (or just $f(x,y)$)}
\label{exampleTwoDice1Table1}
\renewcommand{\arraystretch}{1.4}
\renewcommand{\tabcolsep}{0.3cm}
\begin{tabular}{c c|c c c c c c c c c c c}
&&&&&&$X$ \\ \hline
&& 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\ \hline
&0 & $\frac{1}{36}$ & 0 & $\frac{1}{36}$ & 0 & $\frac{1}{36}$ & 0 &
$\frac{1}{36}$ & 0 & $\frac{1}{36}$ & 0 & $\frac{1}{36}$ \\
&1 & 0 & $\frac{2}{36}$ & 0 & $\frac{2}{36}$ & 0 & $\frac{2}{36}$ & 0 &
$\frac{2}{36}$ & 0 & $\frac{2}{36}$ & 0 \\
&2 & 0 & 0 & $\frac{2}{36}$ & 0 & $\frac{2}{36}$ & 0 &$\frac{2}{36}$ & 0 & $\frac{2}{36}$ & 0
& 0 \\
$Y$&3 & 0 & 0 & 0 &  $\frac{2}{36}$ & 0 &  $\frac{2}{36}$ & 0 &  $\frac{2}{36}$
& 0 & 0 & 0 \\
&4 & 0 & 0 & 0 & 0 &  $\frac{2}{36}$ & 0 &  $\frac{2}{36}$ & 0 & 0 & 0 & 0 \\
&5 & 0 & 0 & 0 & 0 & 0 & $\frac{2}{36}$ & 0 & 0 & 0 & 0 & 0 \\
\end{tabular}
\end{table}
\end{example}
\begin{itemize}
  \item The joint pmf completely defines the probabilities of all combinations
  of $X$ and $Y$.
  $$P((X,Y) \in A) = \sum_{(x,y) \in A}f(x,y)$$
  \item From joint pmf of example \ref{exampleTwoDice1}, a probability that sum
  of two dices is in range $(2,5]$, and that at same time, the difference of
  two dices is in range $[0,1]$. $$P(2 < X \leq 5, 0 \leq Y \leq 1) =
  \frac{2}{36} + 0 + \frac{2}{36} + 0 + \frac{1}{36} + 0 = \frac{5}{36}$$
  \item The expected value is defined as
  $$E(g(x,y)) = \sum_{x,y \in \ureal^2}g(x,y)f(x,y)$$
  \item Expected value from joint pmf of example \ref{exampleTwoDice1}
  \begin{equation*}
  \renewcommand{\arraystretch}{1.6}
  \begin{array}{ll}
  E(XY) & = \sum_{(x,y) \in \ureal^2}xyf(x,y) \\
  & = 2*0*\frac{1}{36}+4*0*\frac{1}{36}+\ldots+12*0*\frac{1}{36}\\
  &~~ +3*1*\frac{2}{36}+5*1*\frac{2}{36}+\ldots+7*5*\frac{2}{36} \\
  &=13\frac{11}{18}
  \end{array}
  \end{equation*}
  \item Them marginal pmf specifies the univariate distribution of one
  component of $(X,Y)$ over all possible values of the other component.
\end{itemize}
\begin{theorem}
Let $(X,Y)$ be discrete bivariate vector with joint pmf $f_{X,Y}(x,y)$. The
marginal pmf's of it are given by
$$f_X(x) = \sum_{y \in \ureal}f_{X,Y}(x,y))$$
$$f_Y(y) = \sum_{x \in \ureal}f_{X,Y}(x,y))$$
\end{theorem}
\begin{itemize}
  \item From example \ref{exampleTwoDice1}, we get marginal pdf seen
  in table \ref{exampleTwoDice1MarginalPmfTable1} for random variable $Y$ of
  joint pmf.
  \begin{table}[h]
  \caption{Probabilities of marginal pmf $f_{Y}(y)$}
  \label{exampleTwoDice1MarginalPmfTable1}
  \renewcommand{\arraystretch}{1.4}
  \renewcommand{\tabcolsep}{0.3cm}
  \begin{tabular}{c|c c c c c c|c}
  Y & 0 & 1 & 2 & 3 & 4 & 5 & $\sum$ \\ \hline
  $P(Y=y)$ & $\frac{6}{36}$ & $\frac{10}{36}$ & $\frac{8}{36}$ & $\frac{6}{36}$
  & $\frac{4}{36}$ & $\frac{2}{36}$ & $\frac{36}{36} = 1$
  \end{tabular}
  \end{table}
  \item Note: Marginal pmf's do not completely specify the probabilities for
  all combinations of $(X,Y)$
  \item For continuous bivariate random variables, the joint pdf is defined
  correspondingly
\end{itemize}
\begin{definition}
A function $f(x,y)$ ($\ureal^2 \rightarrow \ureal$) is called the joint
pdf of the continuous random variable $(X,Y)$, if for every $A \in \ureal^2$
we have
$$P((X,Y) \in A) = \iint_{A}f(x,y)\ud x \ud y$$ %TODO: Better way to mark
% integral lower part for dual integral?
The difference to univariate case is that the integration is done over
2-dimensional set $A$. The expected value is defined as
$$E(g(X,Y)) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,y)f(x,y) \ud x
 \ud y$$
 The marginal pdf's are defined as before, but the sums are replaced by
 integrals
 \begin{equation*}
 \renewcommand{\arraystretch}{1.6}
 \begin{array}{ll}
 f_X(x) = \int_{-\infty}^{\infty}f(x,y) \ud y & -\infty < x < \infty \\
 f_Y(y) = \int_{-\infty}^{\infty}f(x,y) \ud x & -\infty < x < \infty
 \end{array}
 \end{equation*}
 Any function satisfying rules a) and b) bellow is a joint pdf for some random
 variable $(X,Y)$.
 \begin{itemize}
   \item[a)] $f(x,y) \geq 0$ for all $x,y \in \ureal^2$
   \item[b)] $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y) \ud x
   \ud y = 1$
 \end{itemize}
\end{definition}
\begin{example}
\label{exampleJointPdf1}
Define a joint pdf by
\begin{equation*}
\begin{array}{lll}
& f(x,y)=6xy^2 & 0 < x < 1 \text{ and } 0 < y < 1 \\
\text{and} & f(x,y) = 0 & \text{otherwise}
\end{array}
\end{equation*}
%TODO: Some example 3-d plot for joint pdf in notes 8 page 6. Is it needed?
Is this a valid pdf?
 \begin{itemize}
   \item[a)] $f(x,y) = 6xy^2 \geq 0$ for all $x,y \in \ureal^2$
   \item[b)] Is the volume under the curve 1?
   \begin{equation*}
   \renewcommand{\arraystretch}{1.6}
   \begin{array}{ll}
   \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x,y) \ud x \ud y & =
   \int_{0}^{1}\int_{0}^{1}f(x,y) \ud x \ud y \\
   & = \int_{0}^{1}\int_{0}^{1}6xy^2 \ud x \ud y \\
   & = \int_{0}^{1}3y^2 \ud y \\
   & = \big|_{0}^{1}~ 3*\frac{1}{3}y^3 \\
   & = \big|_{0}^{1}~ y^3 \\
   & = 1^3 - 0^3 = 1 \\
   \end{array}
   \end{equation*}
   %TODO: Some steps of solving were passed in notes 8 page 6. Are needed?
   The volume under curve is 1, meaning it is a proper pdf.
 \end{itemize}
Lets compute some probabilities:
\begin{equation*}
   \renewcommand{\arraystretch}{1.6}
   \begin{array}{ll}
   P(X < 0.5, Y < 0.5) & = \int_{0}^{\frac{1}{2}}\int_{0}^{\frac{1}{2}}6xy^2 \ud
   x \ud y \\
   & = \int_{0}^{\frac{1}{2}}\big|_{0}^{\frac{1}{2}}3x^2y^2 \ud y \\
   & = \int_{0}^{\frac{1}{2}}3\frac{1}{4}y^2 \ud y \\
   & = \int_{0}^{\frac{1}{2}}\frac{3}{4}y^2 \ud y \\
   & = \big|_{0}^{\frac{1}{2}}\frac{3}{12}y^3 \ud y \\
   & = \frac{1}{4}\left (\frac{3}{12} \right )^3 - 0 = \frac{1}{4*8} =
   \frac{1}{32}
   \end{array}
   \end{equation*}
The marginal pdf of $X$ is for $0 < x < 1$
\begin{equation*}
   \renewcommand{\arraystretch}{1.6}
   \begin{array}{ll}
   f_X(x) & = \int_{-\infty}^{\infty}f(x,y) \ud y\\
   & = \int_{0}^{1}6xy^2 \ud y \\
   & = \big|_{0}^{1} 2xy^3 \\
   & = 2x*1^3-2x*0^3 = 2x
   \end{array}
   \end{equation*}
Example probability for marginal pdf
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
P(\frac{1}{2} < x < \frac{3}{4}) &=\int_{\frac{1}{2}}^{\frac{3}{4}}f_X(x) \ud x\\
& =\int_{\frac{1}{2}}^{\frac{3}{4}}2x \ud x \\
& =\big|_{\frac{1}{2}}^{\frac{3}{4}} x^2\\
& = (\frac{3}{4})^2-(\frac{1}{2})^2 \\
& = \frac{9}{16}-\frac{4}{16} = \frac{5}{16}
\end{array}
\end{equation*}
\end{example}
\begin{itemize}
  \item The joint cdf is defined by
  $$F(x,y) = P(X \leq x, Y \leq y)\text{ for all } (x,y) \in \ureal^2$$
  \item Not convenient for discrete random variables, but for continuous case,
  we have
  $$F(x,y) = \int_{-\infty}^{x}\int_{-\infty}^{y}f(s,t) \ud s
 \ud t $$
 $$\frac{\ud F(x,y)}{\ud x \ud y} = f(x,y)$$
\end{itemize}
\chapter{Conditional distributions}
\begin{itemize}
  \item Often two random variables are related.
  \item E.g. $X =$ height and $Y$ is the weight of a person.
  \item It is more likely, that $Y > 80$kg, if $X=180$cm, than if $X=150$cm.
  \item $X$ gives some information on $Y$, but it does not tell $Y$ exactly.
  
  \item \textbf{Note} If the relationship between $X$ and $Y$ would be exact,
  then we would have $Y=g(X)$ and we could analyze the data by considering $Y$ as a
  transformation of $X$.
  
  \item Sometimes $Y$ does not tell anything about $X$.
  \item We analyze such cases using the conditional distributions, which apply
    the previously presented idea about conditional probability to probability
    distribution.
\end{itemize}
\begin{definition}
Let $(X,Y)$ be discrete bivariate random vector with joint pmf $f(x,y)$ and
marginal pmf's $f_X(x)$ and $f_Y(y)$.

For any $x$ such that $P(X=x)=f_X(x)>0$, the conditional pmf of $Y$ given that
$X=x$ is the function of $y$ denoted by $f(y \mid x)$ and defined by
$$f(y \mid x) = P(Y=y \mid X=x) = \frac{f(x,y)}{f_X(x)}$$
Correspondingly for any $y$ such that $P(Y=y)>0$, the conditional $pmf$ of $x$
given that $Y=y$ is
$$f(x \mid y) = P(X=x \mid Y=y) = \frac{f(x,y)}{f_Y(y)}$$
\textbf{Note} $f(y|x)$ is a pmf since
\begin{itemize}
  \item[1)] $f(y|x) \geq 0$ for all $y$ because $f(x,y)\geq 0$ for all $x,y$.
  \item[2)] $\sum_{y \in Y}f(y \mid x) = \frac{\sum_{y}f(x,y)}{f_X(x)} =
  \frac{f_X(x)}{f_X(x)} = 1$
\end{itemize}
See example 1.24. in \verb#notes.pdf#. %TODO: Should the actual example be here?
\end{definition}
\begin{itemize}
  \item The extension to continuous random variables is given next.
\end{itemize}
\begin{definition}
Let $(X,Y)$ be a continuous random variable with joint pdf $f(x,y)$ and marginal
pdf's $f_X(x)$ and $f_Y(y)$. For any $x$ such that $f_X(x)>0$, the conditional
pdf of $Y$ given that $X=x$ is the function of $y$ denoted by $f(y \mid x)$ and
defined by
$$f(y \mid x) = \frac{f(x,y)}{f_X(x)}$$
%TODO: Is the \frac{P(X=x, Y=Y)}{P(X=x)} part needed? In notes 9 page 3.
For any $y$ such that $f_Y(y) > 0$, the conditional pdf that $Y=y$ is
$$f(x \mid y) = \frac{f(x,y)}{f_Y(y)}$$
Computing $\int_{-\infty}^{\infty}f(y \mid x) \ud y$, which will be 1, justifies
that $f(y \mid x)$ is a pdf.
\end{definition}
\begin{example}
\label{exampleConditionalPdf1}
Using the joint pdf form the excercise \ref{exampleJointPdf1}
\begin{equation*}
\begin{array}{lll}
& f(x,y)=6xy^2 & 0 < x < 1 \text{ and } 0 < y < 1 \\
\text{and} & f(x,y) = 0 & \text{otherwise}
\end{array}
\end{equation*}
Also, the marginal pdf for $X$ is
$$f_X(x) = 2x$$
The conditional pdf $f(y \mid X=0)$ is undefined, since then, $f_X(x)=2*0= 0$.
$$f(y|X=1) = \frac{f(1,y)}{f_X(1)} = \frac{6*1*y^2}{2*1} = 3y^2$$
This is proper pdf since 
$$\int_{-\infty}^{\infty}f(y \mid X=1) \ud y 
= \int_{0}^{1}3y^2 \ud y
= \big|_{0}^{1}\frac{1}{3}3y^3 = 1-0 = 1$$
%TODO: Is the plot from notes 9 page 4 needed?
Corresponding cdf is
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
F_{Y \mid X=1}(y \mid x=1) & = \int_{-\infty}^{y}f_{Y \mid X=1}
(t \mid x=1) \ud t \\
& = \int_{0}^{y} 3t^2 \ud t \\
& = \big|_{0}^{y}t^3 \\
& = y^3 - 0^3 = y^3
\end{array}
\end{equation*}
\begin{equation*}
\renewcommand{\arraystretch}{1.6}
\begin{array}{ll}
P(Y > 0.5 \mid X=1) & = 1-P(Y \leq 0.5 \mid x=1) \\
& = 1-F_{Y \mid X=1}(0.5 \mid x=1) \\
& = 1 - 0.5^3 = \frac{7}{8}
\end{array}
\end{equation*}
\end{example}
\begin{itemize}
  \item The conditional expected value of $Y$ given that $X=x$ is the expected
  value based on the conditional distribution:
  \begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	E(Y \mid X=x) = E(Y \mid x) = \sum_{y \in \mathcal{Y}}y f(y \mid x) & \text{if
	$y$ is discrete } \\
	E(Y \mid X=x) = E(Y \mid x) = \int_{-\infty}^{\infty} y f(y \mid x) \ud y &
	\text{if $y$ is continuous }
	\end{array}
  \end{equation*}
  \item For $g(Y)$ we have correspondingly
  \begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	E(g(Y) \mid x) = \sum_{y \in \mathcal{Y}}g(y) f(y \mid x) & \text{if
	$y$ is discrete } \\
	E(g(Y) \mid x) = \int_{-\infty}^{\infty} g(y) f(y \mid x) \ud y &
	\text{if $y$ is continuous }
	\end{array}
  \end{equation*}
  \item $E(Y \mid X)$ is ``The best guess of $Y$ we could make given the
  knowledge on $X$''
  \item The continuous variance of $Y$ given that $X=x$ is
  \begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	Var(Y \mid x) & = E\left( \left( Y - E(Y \mid x)\right)^2 \right)\\
	& = E(Y^2 \mid x) - \left[E(Y \mid x)\right]^2
	\end{array}
  \end{equation*}
\end{itemize}
\begin{example}
Using the conditional pdf from example \ref{exampleConditionalPdf1},
well get following conditional expected value and variance for $Y$ on condition
that $X=1$ % TODO: or $X \leq 1$?
\begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	E(Y \mid X=1) & = \int_{-\infty}^{\infty} y f(y \mid X=1) \ud y \\
	& = \int_{0}^{1} y 3y^2 \ud y \\
	& = \int_{0}^{1} 3y^3 \ud y = \frac{3}{4} \big|_0^1 y^4 = \frac{3}{4}(1^4-0^4)
	= \frac{3}{4}
	\end{array}
  \end{equation*}
  
  \begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	E(Y^2 \mid X=1) & = \int_{-\infty}^{\infty} y^2 f(y \mid X=1) \ud y \\
	& = \int_{0}^{1} y^2 3y^2 \ud y \\
	& = \int_{0}^{1} 3y^4 \ud y = \frac{3}{5} \big|_0^1 y^5 = \frac{3}{5}(1^5-0^5)
	= \frac{3}{5}
	\end{array}
  \end{equation*}
  
  \begin{equation*}
	\renewcommand{\arraystretch}{1.6}
	\begin{array}{ll}
	Var(Y \mid X=1) & = E(Y^2 \mid X=1) - \left[E(Y \mid X=1)\right]^2 \\
	& = \frac{3}{5} - \left(\frac{3}{5}\right)^2 \approx 0.0375
	\end{array}
  \end{equation*}
  
  $$sd(Y \mid x=1) = \sqrt{0.0375} = 0.194$$
\end{example}
\begin{theorem}
Let X and Y be any two random variables. Then
$$E(X) = E_Y(~\underbrace{E_{X \mid Y}(X \mid Y)}_{g(y)}~)$$
and
\beq
Var(X) = E_Y(~\underbrace{Var_{X \mid Y}(X \mid
Y)}_\text{function of $Y$, $g(y)$} ~)+Var_Y(~\underbrace{E_{X \mid Y}(X \mid
Y)}_\text{function of $Y$, $g(y)$}~)
\eeq
\textbf{Illustration} See 1.27 in \verb#notes.pdf#.
\end{theorem}
\end{document}