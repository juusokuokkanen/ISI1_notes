%%%%%%%%  Document class  %%%%%%%%%%%%
\documentclass[10pt, twoside, a4paper]{book}

%%%%%%%%  Packages   %%%%%%%%%%%%%%%%
\usepackage{natbib}
%\usepackage{d:/laurim/biometria/lshort/src/lshort}
%\usepackage{numline}
%\usepackage{lineno}
\bibpunct{(}{)}{,}{a}{}{,}
\usepackage{amsmath,amssymb}
\usepackage{bm} 
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{color}

%\newenvironment{Rcode}[1]%
%{\scriptsize{\ttfamily{\bf The R-code for #1}} \\
%\ttfamily \noindent\ignorespaces }
%{\par\noindent%
%  \ignorespacesafterend}
  
\newenvironment{Rcode}[1]%
{\ttfamily{\bf The R-code for #1}
\scriptsize \\\par\verbatim}
{\endverbatim\par} 

\newenvironment{Rcode2}%
{\scriptsize \par\verbatim}%
{\endverbatim\par}

\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage[nolists]{endfloat}
\usepackage{amsthm}

\usepackage{times}
%\usepackage[T1]{fontenc}

\newcommand{\foldPath}{c:/laurim/tex/}

\newcommand{\beq}{\begin{equation*}}
\newcommand{\eeq}{\end{equation*}}

\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\sd}{sd}

\theoremstyle{definition}

\newtheorem{example}{Example}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
%\bibpunct{(}{)}{,}{a}{}{,}
\bibliographystyle{\foldPath saf}
%\bibliographystyle{alpha}
%\bibliographystyle{\foldPath wiley}
%\bibliographystyle{unsrt}

%%%%%%%%  Page Setup %%%%%%%%%%%%%%%%%%
%\topmargin      0pt
%\headheight     0pt 
%\headsep        0pt 
%\textheight   648pt 
%\oddsidemargin  0pt
%\textwidth    468pt 

%%%%%%%% Document %%%%%%%%%%%%%%%%%%%
\begin{document}
\pagenumbering{Roman}
\setlength{\baselineskip}{16pt}

%% Front matter
%\title{Forest biometrics with examples in R}
\title{Introduction to statistical inference 1}

\author{Lauri Meht\"atalo\\
	   University of Eastern Finland \\ School of Computing
	   }

\date{\normalsize \today}
\maketitle
\tableofcontents

%\section*{Preface}

%These are lecture notes for course Biometrics: R-statistics in Forest Sciences. In writing these notes, I have used several sources. In writing chapter 1, the main sources have been the second edition of the classical book ``Statistical Inference'' by George Casella and Roger L Berger\citep{Casellaandberger2002} and ``Methods for Forest Biometrics'' by Juha Lappi  \citep{Lappi1993}. In addition, I have used lecture notes of courses on mathematical statistics (by Eero Korpelainen) and statistical inference (by Jukka Nyblom). For chapters 2 and 3, the main sources have been the lecture notes of Jukka Nyblom on Regression analysis and Linear models, the book of Jose Pinheiro and Douglas Bates on Linear models with R \citep{Pinheiroandbates2000}, and the book ``Generalized, Linear and Mixed Models'' by CharlesMcCulloch and Shayle R Searle \citep{McCullochandsearle2001}. For the GLMs and GLMMs, I have used \citet{McCullochandsearle2001}, too. The last chapter on models systems is based mainly on book ``Econometric analysis'' by William H Greene \citep{Greene1997}. In addition, good sources of information have been the lecture notes of Annika Kangas on Forest Biometrics, the books of of Julian Faraway \citep{Faraway2004, Faraway2006}, Keith E Muller and Paul W Stewart \citep{Mullerandstewart2006}, and William N. Venables and Brian D. Ripley \citep{Venablesandripley2002}.   

\mainmatter
\pagenumbering{arabic}

\chapter{Preliminaries}
\section{Introduction}
\begin{itemize}
  \item Field of statistics builds on probability theory
  \begin{quotation}
  \textit{``You can, for example, never foretell what any one man will do, but
  you can say with precision what an average number will be up to. Individuals vary,
  but percentages remain constant. So says the statistician.''} - Sherlock Holmes
  \end{quotation}
  \item The paragraph includes the important ideas of the statistical model:
  \begin{itemize}
    \item The percentage $p$ = the model of the process or underlying
    population
    \item The behavior of individuals = data
  \end{itemize}
  \item Assuming a constat probability $p$ may be a too simplistic or naive
  assuption, and may be replaced by more realistic one where p is a function of
  the propabilities of the individual and the context where s/he is.
  \item We also need to specify a model, for the variability of the
  individuals around the $p$ to complete the model formulation.
  \begin{itemize}
    \item A crude summary if the variance-covariance matrix of the
    observations.
    \item A complete definition is done by specifying the joint distribution
    of all individuals.
  \end{itemize}
  \item We also may want to estimate how accurately we finally estimated $p$
  writing the available data
  \item The theoretical process that generates the data is called
  \begin{itemize}
    \item Statistical model  or (tilastollinen malli)
    \item Stochastic process  or (stokastinen prosessi)
    \item Random process or (satunnaisprosessi)
  \end{itemize}
  \item The process is random/stochastic because the ``man'' do not behave
  exactly according to model.\footnote{This is what is done on this part of the
  course (ISI1).}
  \begin{itemize}
    \item Probability calculus and the theory of random variables provide
    tools to formulate and understand such models.
  \end{itemize}
  \item Once model has been formulated or specified (muotoiltu), observed data
  can be used to\footnote{This is what is done on second part of the course
  (ISI2)}
  \begin{itemize}
    \item estimate model parameters
    \item evaluate the model fit (mallin sopivuus)
    \item evaluate the inaccuracy related to the estimated model parameters
  \end{itemize}
  \item When talking about models, we can talk about
  \begin{itemize}
    \item True model (Tosi malli)
    \item Estimated model (Estimoitu malli)
    \item True model always stays the same, but as data used to formulate the
    estimated model gets larger, the estimated model gets closer to true model.
    \item See example R-scipt \verb#regsimu.R#
  \end{itemize}
\end{itemize}
\section{Set theory}
\begin{itemize}
	\item Consider a statistical experiment (e.g. rolling a dice, measuring the
	diameter of a tree, tossing a coin, measuring the photosynthetic activity in plant etc.)
\end{itemize}
\begin{definition}
All possible outcomes of a particular experiment (koe) form a set
(joukko) called sample space (otosavaruus), denoted by S. For example:

% \textbf{Examples} % TODO: Can this be done better?
\begin{itemize}
  \item[A] Toss of a coin; $ S = \{H, T\} $
  \item[B] Reaction time, Waiting time; $ S = [0, \infty) $
  \item[C] Exercise score of this course; $ S = \{0,1,2,\ldots,210\} $
  \item[D] Number of points (events) within fixed area; $ S = \{0,1,2,\ldots $
  \item[E] CO\textsubscript{2} uptake within 0.5 hours in fixed area plot; $ S =
  (-\infty, \infty) $
  \item[F] Waiting time up to one hour (in minutes); $ S = [0, 60) $
\end{itemize}

Sample space can be countable (numeroituva) or uncountable (ylinumeroituva). If
the elements of a sample space can be put into one-to-one correspondence with a
finite subset of integers, the space is countable. Otherwise, it is uncountable.
\end{definition}
\begin{itemize}
  \item Note: Examples A and C before are countable, the others are uncountable
  \item Note: If the waiting time in G are rounded to the
  minute / second / millisecond / microsecond, the sample space becomes
  countable.
\end{itemize}
\begin{definition}
An event (tapaus) is any collection of possible outcomes of an experiment,
meaning it is a subset of S. Event A is said to occur, if the outcome of the experiment is in set A.
\end{definition}
\begin{example}
Draw a card from standard deck.
$$S = \{ \heartsuit , \diamondsuit , \clubsuit, \spadesuit \}$$
One possible event is $A = \{ \heartsuit , \diamondsuit\}$.
Another possible event is $B = \{ \diamondsuit , \clubsuit, \spadesuit \}$. The
union (unioni) of the two events includes all elements of both
$$A \cup B = \{ \heartsuit , \diamondsuit , \clubsuit, \spadesuit \}$$
The intersection (leikkaus) includes elements that are common to both events
$$A \cap B = \{ \diamondsuit \}$$
The complement (komplementti) of a set includes al elements of $S$ that are not
included in $A$
$$A^c = \{ \clubsuit, \spadesuit \}$$
Events $A$ and $B$ are said to be disjoint (erillisi\"a), if 
$$A \cap B = \emptyset$$
Number of events $A_1$, $A_2$, $A_3$, \ldots are said to be pairwise disjoint,
if
$$A_i \cap A_j = \emptyset$$
for all pairs of i, j. In addition, if $\bigcup_{i=1}^{\infty}A_i = S$,
then $A_1$, $A_2$, $A_3$, \ldots defines a partition of the sample space.
\end{example}
\begin{example}
Events $A = \{ \heartsuit , \diamondsuit\}$ and $B = \{\clubsuit, \spadesuit
\}$ are disjoint since 
$$A \cap B = \emptyset$$
Events $A_1=\{\heartsuit,
\diamondsuit \}$, $A_2 = \{ \clubsuit \}$ and $A_3 = \{ \spadesuit \}$
are pairwise disjoint. Also, since $$\bigcup_{i=1}^{3} = A_1 \cup A_2 \cup A_3 =
\{\heartsuit , \diamondsuit , \clubsuit, \spadesuit\} = S$$
they are also partiotion of S.
\end{example}
% TODO: Do we something about the dice example? In pdf notes, just example of
% sample space of dice throw experiment, and how it is numerated. {1,2,3,4,5,6}
\begin{definition}
Probability (todennäköisyys)

If a certain experiment is performed number of times (or infinite number of
times), it may lead to different outcome, which is an event of the sample space.
This frequency of outcome of an event is called probability.

For an event $A \subset S$ in an experiment, notation $P(A)$ (or $Pr(A)$)
specifies the probability of outcome / event $A$.
\end{definition}
\begin{theorem}
Axioms of probability
\begin{itemize}
  \item[1.]For every event $A$, $P(A) \neq 0$ (meaning every event is possible)
  \item[2.]$P(S) = 1$ (because something will be observed)
  \item[3.]For a sequence of pairwise disjoint
  events,$$P(\bigcup_{i=1}^{\infty}A_i) = \sum_{i=1}^{\infty}P(A_i)$$
\end{itemize}
\end{theorem}
\begin{example}
%TODO: There were some diagrams related to these examples, that are not yet
% added. Should they be?
Assume we have two events $A_1$, $X \in A_1$ and $A_2$, $X \in A_2$, which have
probabilities $P(A_1) = 0.2$ and $P(A_2) = 0.3$.

%\textbf{Case 1}%
If the events are disjoint ($A_1 \cap A_2 = \emptyset$),
% then
probability for the union of events is $$P(A_1 \cup A_2) = P(A_1) + P(A_2) = 0.2
+ 0.3 = 0.5$$

%\textbf{Case 2}%
If events are not disjoint ($A_1 \cap A_2 \neq \emptyset$), then
$$P(A_1 \cup A_2) \neq P(A_1) + P(A_2) = 0.2 + 0.3 = 0.5$$
\end{example}
\begin{example}
In a fair deck, define events $$A_1 = \{\heartsuit\}, A_2 = \{\diamondsuit\},
A_3 = \{\clubsuit\}, A_4 = \{\spadesuit\}$$which have probabilities
$$P(A_1) = P(A_2) = P(A_3) = P(A_4) = 1/4$$
Events $A_1 \ldots A_4$ are disjoint. Therefor,
$$B = A_1 \cup A_2 = \{\heartsuit, \diamondsuit\}$$
$$P(B) = P(A_1 \cup A_2) = P(A_1) + P(A_2) = 1/4 + 1/4 = 1/2$$
\end{example}
\begin{theorem}
Consider events A and B
\begin{itemize}
  \item[1.]$P(A^c) = 1 - P(A)$
  \item[2.]$P(B \cap A^c) = P(B) - P(A \cap B)$
  \item[3.]$P(A \cup B) = P(A) + P(B) - P(A \cap B)$
  \item[4.]If $A \subset B$, then $P(A) \leq P(B)$
\end{itemize}
\textbf{Note} Consider case 3 of theorem \ref{Theorem2}
$$P(A \cup B) = P(A) + P(B)-P(A \cap B)$$
$$P(A \cup B) \leq 1$$
$$P(A) + P(B)-P(A \cap B) \leq 1$$
$$P(A \cap B) \geq P(A) + P(B)-1$$
This equation is called the Bonferroni inequality. Idea is, that if we have
intersection of two events ($A$ and $B$), the probability of the intersection
can be shown to be higher or equal than the right term.

$$P(A \cap B) \geq P(A) + P(B)-1$$
Supose $A$ and $B$ are two events that occur with probability $P(A) = P(B) =
0.975$. Then $P(A \cap B) = 0.975 + 0.975 - 1 = 0.95\ldots$ 


\label{Theorem2}
\end{theorem}
\begin{theorem}
\begin{itemize}
  \item[a)] $P(A) = \sum_{i=1}^{\infty} P(A \cap C)$ for any partition
  $C_1, C_2, \ldots$
  \item[b)]$P(\bigcup_{i=1}^{\infty}A_i) \leq \sum_{i=1}^{\infty}P(A_i)$, for
  any sets $A_1, A_2, \ldots$
\end{itemize}
\end{theorem}
\begin{definition}
IF $A$ and $B$ are events in a sample space $S$ and $P(B) > 0$, then
conditional probability (ehdollinen todenn\"akoisyys) of $A$ given $B$ is
$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$
\end{definition}
\begin{example}
\label{clinicalExample1}
Clinical trial. Assume  that we know the probabilities that are represented in
table \ref{probTable1}.
Let event $A$ be the event ``the patient recovered'' with sample space $S =
\{ ``OK", ``NOT~OK"\}$, and event B is the event ``The patient was treated
with placebo'' with $S = \{``YES'', ``NO''\}$.

Now $P(A \cap B)$ can be computed directly using the table \ref{probTable1}, and
it is
$$P(A \mid B) = P(``OK'' \mid ``Placebo'') = \frac{P(A \cap B)}{P(B)} =
\frac{0.160}{0.227} \approx 0.70$$

\begin{table}
\caption{Probabilities related to clinical trial. Four different drugs and the
probabilities that patient got or did not got ok after using it.}
\label{probTable1}
\begin{tabular}{| r | r | r | r | r | r |}
\hline
{ } & Drug1 & Drug2 & Drug3 & Placebo & Total \\
\hline
OK & 0.120 & 0.087 & 0.147 & 0.160 & 0.513 \\
\hline
{NOT OK} & 0.147 & 0.167 & 0.107 & 0.067 & 0.487 \\
\hline
Total & 0.267 & 0.253 & 0.253 & 0.227 & 1.000 \\
\hline
\end{tabular}
\end{table}
\end{example}
\begin{definition}
%TODO: Can equations here be done more better?
Bayes rule

Assuming we have event $A$ and $B$,
$$P(A|B) = \frac{P(A \cap B)}{P(B)} ~~\|* P(B)$$
if $P(B) > 0$
$$P(A \cap B) = P(B)P(A|B)$$

And the other way around
$$P(B|A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A \cap B)}{P(A)} ~~\|* P(A)$$
if $P(A) > 0$
$$P(A \cap B) = P(A)P(B|A)$$
and by using the earlier $P(A \cap B) = P(B)P(A|B)$
$$P(B)P(A|B) = P(A)P(B|A) ~~\|: P(B)$$
$$P(A|B) = \frac{P(A)P(B|A)}{P(B)}$$
This is known as Bayes rule.
\end{definition}
\begin{definition}
Two events $A$ and $B$ are independent, if
$$ P(A \cap B) = P(A)P(B)$$
If two events are known to be independent, we can compute $P(A \cap B)$ as
$P(A)P(B)$
\end{definition}
\begin{example}
In example \ref{clinicalExample1} we had $P(A) = 0.513$, $P(B)=0.227$ and
$P(A \cap B) = 0.160$. By calculating
$$P(A)P(B) = 0.513 * 0.227 = 0.116 \neq 0.160 = P(A \cap B)$$
we can determine that events $A$ and $B$ are not independent.
\end{example}
\chapter{Random variables}
\begin{definition}
A random variable (RV for short) (satunnaismuuttuja) is a function from sample
space S into the real numbers.\\
\textbf{Note} If S includes only real numbers, the definition of the RV is
implicit. Some examples of this in table \ref{tableImplicitEventRV1}. Examples
of events that not implicitly connected to random variables in table
\ref{tableImplicitEventRV2}
\newline
%TODO: Can we make the aligment of table cells better?
\begin{table}
\caption{Examples of implicit connection between event and random variable}
\label{tableImplicitEventRV1}
\begin{centering}
\begin{tabular}{p{6cm} | p{6cm}}
\textbf{Experiment} & \textbf{Event = Random variable} \\ \hline
Toss of two dice. & $X$ = Sum of two numbers \\ \hline
Estimated regression from n observations from certain true model.
& $X$ = Estimate of slope coefficient $\hat{\beta}_2$ \\ \hline
Event location in space. & $X$ = \# of events in certain fixed subset of the
space.
\end{tabular}
\end{centering}
\end{table}

\begin{table}
\caption{Examples of implicit connection between event and random variable}
\label{tableImplicitEventRV2}
\begin{centering}
\begin{tabular}{p{3cm} | p{5cm} | p{4cm}}
\textbf{Experiment} & \textbf{S} & \textbf{RV} \\ \hline
Toss of a coin & $\{``H'', ``T''\}$ & \begin{equation*}
X_1 = \left\{
\begin{array}{rl}
1 & \text{if Head}, \\
0 & \text{if Tails}
\end{array} \right
\end{equation*}
or
\begin{equation*}
X_2 = \left\{
\begin{array}{rl}
10 & \text{if Head}, \\
2 & \text{if Tails}
\end{array} \right
\end{equation*}
\\ \hline
Health status of tree & $\{``Healthy", ``Sick", ``Dead"\}$ & \begin{equation*}
X = \left\{
\begin{array}{rl}
1 & \text{if Healthy}, \\
2 & \text{if Sick}, \\
3 & \text{if Dead}
\end{array} \right
\end{equation*}
or we can define two random variables
\begin{equation*}
X_1 = \left\{
\begin{array}{rl}
0 & \text{if Healthy or Dead}, \\
1 & \text{if Sick}
\end{array} \right
\end{equation*}
\begin{equation*}
X_2 = \left\{
\begin{array}{rl}
0 & \text{if Healthy or Sick}, \\
1 & \text{if Dead}
\end{array} \right
\end{equation*}
\\
\end{tabular}
\end{centering}
\end{table}
The probability function of the original sample space is defined ot the RV as
follows:

Consider the sample space $S = {S_1, S_2, \ldots ,S_n}$, and has probability
function P. Define RV $X$ with sample space / range / support set
$$ \chi = \{x_1, x_2, \ldots , x_n \}$$ 
% TODO: Is there any better letter for RV support? Can't use plain X.
$$P_X(X = x_i) = P(\{ s_j \in S : X(s_j) = x_i \})$$
\textbf{Note} About the notations: Uppercase (capital) letters are used
for a random variables and lowercase letters for the realized value or range. 
$X = x$ means, that RV $X$ gets value $x$.
\end{definition}
\newpage
\begin{example}
\label{exampleTreeCoinToss}
Tree tosses of a coin. Let RV $X$ be the \# of heads, or RV $Y$ is the number of
tails. For RV $X$, well get space $$\chi = \{0,1,2,3\}$$ See table
\ref{exampleTreeCoinTossTable1} to see how the sample space values are converted to random variable values.
We can then calculate the probabilities that random variable gets certain value
from its support space. For random variable $X$, these probabilities are
%TODO: Better way to add space between tabular and text?
\newline
\newline
\begin{tabular}{r | r r r r}
$x$ & 0 & 1 & 2 & 3 \\
$P(X=x)$ & $\frac{1}{8}$ & $\frac{3}{8}$ & $\frac{3}{8}$ & $\frac{1}{8}$
\end{tabular}
\newline
\newline
Now, we can get the probability of event, that with three coin tosses,
we get only one heads, by getting the probability that random variable $X$
receives value 1 $$P(X=1) = P(\{HTT,~THT,~TTH\}) = \frac{1}{8}+\frac{1}{8}+\frac{1}{8} =
\frac{3}{8}$$
\end{example}
\begin{table}
\caption{Example \ref{exampleTreeCoinToss} experiment's sample
space value conversion to RV values.}
\label{exampleTreeCoinTossTable1}
\begin{tabular}{ r | r  r  r  r  r  r  r  r }
$s_i$ & HHH & HHT & HTH & HTT & THH & THT & TTH & TTT \\
\hline
$X(s_i)$ & 3 & 2 & 2 & 1 & 2 & 1 & 1 & 0 \\
$Y(s_i)$ & 0 & 1 & 1 & 2 & 2 & 1 & 2 & 3 \\
\hline
\end{tabular}
\end{table}
\begin{example}
Waiting time in a phone service. Sample space for experiment is
$$S = [0,60)$$
For RV $X$, space will be
$$\chi = [0, 60)$$
meaning since the original experiment has sample space that is defined by real
number, the experiment directly specifies the random variable
 $$X(s) = s$$
\end{example}
\begin{itemize}
  \item Starting point with any computation of probability is the distribution
  function.
\end{itemize}
\begin{definition}
The culmulative distribution function (kertym\"afunktio) or just distribution
function (cdf for short) is defined as
\begin{equation*}
F_X(x) = P_X(X \leq x)~\text{for all $x$}
\end{equation*}
\end{definition}
\begin{example}
Tossing of 3 coins. Random variable $X$ = the \# of heads. Cumulative
distribution function for RV $X$ can be defined as
\begin{equation*}
F_X(x) = \left\{
\begin{array}{rll}
0 & \text{if } -\infty < x < 0 & x \in (-\infty, 0) \\
\frac{1}{8} & \text{if } 0 \leq x < 0 & x \in [0, 1) \\
\frac{1}{2} & \text{if } 1 \leq x < 0 & x \in [1, 2) \\
\frac{7}{8} & \text{if } 2 \leq x < 0 & x \in [2, 3) \\
1 & \text{if } 3 \leq x < \infty & x \in [3, \infty) 
\end{array} \right
\end{equation*}
%TODO: There is a plot about the cdf and also some specification how value for
% 1/2 and 7/8 are gotten. Are they needed?
\textbf{Notes}
\begin{itemize}
  \item[-] $F_X(x)$ is defined for all $x$!
  %TODO: If the plot for cdf is added, here is additional note.
  %\item[-] $F_X(x) has jumps at x_i \in \chi$. The jumps equal to P(X=x_i).
  \item[-] $F_X(x) = 0$ for $x < 0$ and $F_X(x) = 1$ for $x \geq 3$.
  \item[-] $F_X(x)$ is right-continuous: the value of $F_X(x)$ at the jump is
  the one we get by approaching the jump from the right.
  \item[-] $F_X(x)$ is discontinuous (in this example).
\end{itemize}
\begin{theorem}
\label{rulesForCdfTheorem}
Function $F(x)$ is a cdf if and only if the following conditions hold:
\begin{itemize}
  \item[a)] $\lim_{x \rightarrow -\infty}F(x) = 0$ and $\lim_{x \rightarrow
  \infty}F(x) = 1$.
  \item[b)] $F(x)$ is non-decreasing (kasvava) (meaning value of function
  is either increasing or constant as $x$ increases)
  \item[c)] $F(x)$ is right-continuous, i.e. for every $x_0$ 
  $$\lim_{x \downarrow x_0}F(x) = F(x_0)$$
\end{itemize}
\end{theorem}
\begin{itemize}
  \item If a function is said to be a cdf, it has to fulfill the rules a, b and
  c of theorem \ref{rulesForCdfTheorem}.
  \item If a function fulfills rules a, b and c of theorem
  \ref{rulesForCdfTheorem}, then it can be used as a cdf.
\end{itemize}
\begin{theorem}
A random variable $X$ is said to be continuous, if $F_X(x)$ is a continuous
function of x (meaning the function foes hot have jumps).
\end{theorem}
\textbf{Note} The $X$ in example \ref{exampleTreeCoinToss} was discrete.
\end{example}
\end{document}